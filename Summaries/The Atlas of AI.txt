

The text is a copyright notice and table of contents for the book "Atlas of AI" by Kate Crawford. 

The book explores the power, politics, and planetary costs of artificial intelligence. 

The introduction references a historical example of a horse named Clever Hans that was able to perform complex tasks, highlighting the fascination with intelligence and artificial intelligence. 

The table of contents includes chapters on Earth, Labor, Data, Classification, Affect, State, Power, and Space, as well as a conclusion, coda, acknowledgments, notes, bibliography, and index.

Clever Hans was a horse trained by Wilhelm von Osten to perform advanced intellectual tasks such as counting and spelling. The horse became a sensation in the early 1900s, but many were skeptical of his abilities. An investigative commission led by Carl Stumpf found no evidence of deception but discovered that Hans was actually picking up on subtle cues from his trainer and the questioner. This phenomenon became known as the Clever Hans effect and is an example of the importance of controlling for experimenter effects in animal behavior research.

The Clever Hans effect was discovered by Carl Stumpf and Oskar Pfungst during an investigation into the abilities of Clever Hans, a horse trained by Wilhelm von Osten. The effect occurs when the questioner provides unintentional cues to the subject, leading them to the correct answer. This phenomenon has since been used as a cautionary reminder in machine learning, highlighting the importance of controlling for experimenter effects in animal behavior research and the potential for biases to influence outcomes.

The book explores the question of how intelligence is made and the potential traps that arise. The story of Clever Hans demonstrates that the construction of intelligence involves validation from various institutions and interests, including academia, science, the public, and the market. Two distinct myths are at work in the idea that nonhuman systems can emulate human minds with enough training and that intelligence is natural and distinct from social, cultural, historical, and political forces. These myths have been particularly prevalent in the field of artificial intelligence and have been used to justify relations of domination.

The idea of machines being able to think like humans has been present since the mid-twentieth century, with some AI pioneers believing in the possibility of creating machines that learn like children. However, others, such as Joseph Weizenbaum, argued that this notion is too simplistic and fails to capture the complexity of human intelligence. The dispute over whether human and machine tasks are fundamentally different has been a core issue in the history of artificial intelligence. While some, like John McCarthy, believed that machines could eventually solve any task, others, such as Hubert Dreyfus, pointed out that human intelligence operates differently from computer processing.

AI relies on explicit and formalized processes, leaving out less formal aspects of intelligence that humans rely on. The field of AI has rapidly expanded since the 2000s, with powerful corporations deploying AI systems at a planetary scale. However, the narrow perspective of intelligence in AI, based on Cartesian dualism, limits its recognition of intelligence in non-human forms. This narrow perspective has infected decades of thinking in the computer and cognitive sciences, creating a kind of original sin for the field. The definition of AI is deceptively simple and often associated with specific technologies or companies.

AI can be defined in different ways, including as a technical process involving neural nets or as the attempt to understand and build intelligent entities. Each definition frames how AI is understood, measured, valued, and governed. The author argues that AI is not artificial or intelligent, but rather embodied and material, made from natural resources, human labor, and infrastructures. AI systems are not autonomous or rational, and require extensive training with large datasets or predefined rules.

Artificial intelligence is not just a technical domain, but a product of wider political and social structures that serve existing dominant interests. It reflects and produces social relations and understandings of the world. The term "artificial intelligence" is sometimes rejected in the computer science community, but is still used in marketing and funding applications. The author uses the term to refer to the massive industrial formation that includes politics, labor, culture, and capital, while "machine learning" refers to technical approaches that are also social and infrastructural. The field has been focused on technical breakthroughs, product improvements, and profits.

A narrow analysis of AI as a technology serving convenience obscures the political implications of what is being optimized and for whom. An atlas can help us understand the political nature of AI by presenting a particular viewpoint of the world and linking disparate pieces together differently. An atlas is an act of creativity that combines the aesthetic and epistemic paradigms, undermining the idea that science and art are separate. Cartographic approaches can be helpful in understanding the political implications of AI.

Maps are purposeful and represent collective knowledge, but some maps can also serve as tools of domination. A topographical approach is needed to understand the shifting tectonics of power in AI, accounting for the extractive mining, data capture, and labor practices that sustain it. The AI industry aims to capture the planet in a computationally legible form and create its own centralized maps, superseding other forms of knowing. AI must be understood in a wider context by exploring different landscapes of computation and their connections.

Some in the AI field desire to be the dominant way of seeing, centralizing power and denying the inherently political nature of mapping. This book offers a partial account of the material construction and ideologies of AI, exploring various landscapes of computation and providing alternatives to hegemonic approaches. Transparency is not enough to understand the interconnected systems of power in AI, and a better understanding is gained by engaging with its material architectures, contextual environments, and prevailing politics. The author's thinking is informed by the disciplines of science and technology studies, feminist theory, critical race theory, and geography.

The author of the book, who has experience in technology studies, law, and political philosophy, acknowledges the influence of scholars and colleagues in shaping their perspective on sociotechnical systems. The book focuses on the politics of technology in the AI industry in Western centers of power, but acknowledges the limitations of this perspective and the importance of considering local observations and interpretations. The author challenges the idea of AI's inevitable expansion and highlights the constructed nature of underlying AI visions, which are based on a particular set of beliefs and perspectives.

The contemporary atlas of AI is dominated by a small group of people working in wealthy AI industry cities, and the maps they create are political interventions rather than neutral reflections of the world. The book explores how AI is conceptualized and constructed, and its social and material consequences in various institutions. It takes an expanded view of AI as an extractive industry that relies on exploiting resources, labor, and data at scale. The book is not a technical account of AI, nor an ethnographic study, but a journey to diverse locations to understand the role of AI in the world.

The text discusses the environmental and human impacts of the extractive politics of AI, including the vast amounts of rare earth minerals, oil, and coal required for contemporary computation. It also examines the role of human labor in creating and operating AI, from digital pieceworkers to employees in Amazon warehouses and Chicago meat laborers, and the ways in which AI systems increase surveillance and control for bosses. The coordination of human and machine actions and the controlling of bodies in space and time is also discussed as central to workplace management and AI technologies.

The text explores the increasingly detailed mechanisms of temporal management and the role of data in creating AI models, including personal data that is harvested from publicly accessible digital material. It also discusses the practices of classification in AI systems, including the problematic use of binary gender and essentialized racial categories, and how technical schemas enforce hierarchies and magnify inequity. The chapter examines affect recognition and its history in Papua New Guinea. The current practices of working with data in AI raise profound ethical, methodological, and epistemological concerns.



Chapter 5 discusses Paul Ekman's claim of universal emotional states that can be read from facial expressions, which tech companies are using for affect recognition systems, despite scientific controversy. 

Chapter 6 explores the use of AI systems as a tool of state power shaped by the military past and present, now being used in various sectors. 

AI functions as a structure of power combining infrastructure, capital, and labor, widening existing asymmetries of power, and relies on abstraction and extraction.



Artificial intelligence is a complex concept that includes ideas, infrastructure, industry, power, and a way of seeing, backed by organized capital and vast systems of extraction and logistics. 

It is a physical infrastructure that reshapes the Earth and shifts how the world is seen and understood, and its malleability allows it to be used in various ways. 

Contending with the different aspects of AI and considering their interconnectedness can lead to a new conception of planetary politics, promoting data protection, labor rights, climate justice, and racial equity.

Artificial intelligence is shaping knowledge, communication, and power at the level of epistemology, social organization, and politics. The dominance of a few large-scale AI companies has intensified technocratic power and created downstream effects. We must recognize AI as a political, economic, cultural, and scientific force and ask hard questions about its production and adoption.

The use of AI must be critically examined in terms of whose interests it serves and who bears the greatest risk of harm. Addressing foundational problems of AI requires connecting issues of power and justice, from labor rights to climate change. The book argues for expanding our understanding of the empires of AI in Silicon Valley and making better collective decisions about what should come next.

The author travels from Silicon Valley to Silver Peak, Nevada to learn about artificial intelligence. Along the way, they pass landmarks and locations significant to the global economy and computational infrastructure, including Lawrence Livermore National Laboratory and the world's largest ammunition depot. Silver Peak, an old mining town, is the author's destination, where they hope to find insights into AI.

Silver Peak, Nevada is home to a massive underground lithium lake that is mined for use in artificial intelligence. The history of mining in the region is intertwined with the development of Silicon Valley, with San Francisco's wealth built on the gold and silver extracted from the lands of California and Nevada. Mining's environmental and societal costs are often overlooked in the pursuit of profit.

Georgius Agricola, father of mineralogy, noted in 1555 that the harm caused by mining far outweighs the value of the metals produced. Despite this, extract everything as rapidly as possible was the prevailing mindset during the gold rush era in California. The Central Valley was decimated, but San Francisco drew enormous wealth from the mines, allowing its populace to remain ignorant of the environmental and social impact of mining. However, the technology used in the mines was adapted to create the city's skyscrapers, making them inverted minescapes.

The technology industry, including Apple, Microsoft, Amazon, Facebook, and Google, has become the new supreme interest in San Francisco. While the city has benefited from the tech boom, with luxury cars and venture capital-backed coffee chains, it also has one of the highest rates of street homelessness in the US, which the UN special rapporteur on adequate housing called an unacceptable human rights violation. The chapter will traverse across various places, including Nevada, San Jose, and San Francisco, as well as Indonesia, Malaysia, China, and Mongolia, to show the logics of extraction and the constant drawdown of minerals, water, and fossil fuels undergirded by violence and depletion.

The article discusses the planetary scale of computational extraction and the importance of lithium mining for rechargeable batteries. The only operating lithium mine in the US, Silver Peak, is a site of intense interest to tech tycoons like Elon Musk. Lithium is a crucial element for battery production, with each Tesla Model S requiring about 138 pounds of it. All batteries have a limited lifespan and are discarded as waste.

The article discusses Tesla's role as the world's number-one lithium-ion battery consumer and its need to secure a domestic supply of critical minerals like lithium. The negative environmental impact of the battery supply chain is highlighted, and the article notes that the electric car is not a perfect solution to reducing carbon dioxide emissions. The importance of rechargeable batteries for global computation and commerce is also discussed.

The use of cloud computing for modern life, including artificial intelligence, comes with a material cost that is often overlooked. The mining required for AI encompasses both data and traditional mining, and the supply chain for AI requires an enormous amount of resources from capital, labor, and the Earth. Media and technology are geological processes that contribute to the depletion of nonrenewable resources. Each object in an AI system requires elements that took billions of years to form inside the Earth, and we are using Earth's geological history to serve contemporary technological needs.

The short lifespan of devices, driven by obsolescence cycles, leads to unsustainable extraction practices and e-waste dumping grounds. The lifecycle of an AI system involves many fractal supply chains, including exploitation of human labor and natural resources. The tech sector in San Francisco is built on the same extractivism that powers the AI industry. The extraction of materials for AI, such as lithium, happens in various places around the world, including the Salar in Bolivia. AI extracts far more from us and the planet than is widely known.

AI relies on critical minerals, including rare earth elements, which are found in places like Congo, Mongolia, Indonesia, and Western Australia. These minerals are in short supply and are at a high risk of shortage. Mining for these minerals often leads to local and geopolitical violence.

The extraction of critical minerals used in AI comes with war, famine, and death. Legislation like the Dodd-Frank Act aims to regulate the use of conflict minerals, but the term masks the suffering in the mining sector, which has funded military operations and amounted to modern slavery. Companies like Intel and Apple have been criticized for only auditing smelters, not the actual mines, to determine conflict-free status, and even conflict-free certifications of the tech industry are now under question.

The supply chains of computer components involve numerous suppliers and sub-suppliers, making it challenging to trace the origins of the minerals used. This complexity allows manufacturers to deny responsibility for any exploitative practices in their supply chains. The mining of minerals for technology has negative human and environmental consequences beyond direct conflict zones. Capitalism's structure facilitates ignorance of supply chains, which creates a form of bad faith. The focus on conflict minerals can distract from the broader harms of mining. Mining for technology causes environmental damage and human suffering.

The Baotou mines in China, which contain 70% of the world's reserves of rare earth minerals, have caused significant environmental damage, including toxic black lakes created by waste runoff from ore processing. Rare earth minerals require hazardous processes to extract and produce toxic waste, leading to pollution and extinction of local species. China's dominance in the rare earth minerals market is due to its willingness to accept environmental damage from extraction. The ratio of usable minerals to waste toxins is extreme, with 99.8% of earth removed in rare earth mining being discarded as waste.

The production of rare earth elements in China creates acidic water and radioactive residue. Small islands off the coast of Sumatra in Indonesia produce 90% of the world's tin, which is used in semiconductors. The tin mining process on these islands is completely unregulated, causing destruction to the landscape and posing environmental and health risks to the miners. The tin is sold to middlemen who mix it and sell it to companies like Timah, which supply major tech companies like Samsung, Sony, LG, Foxconn, Apple, Tesla, and Amazon.

To understand the supply chains of AI, one must recognize the global and historical patterns of extraction and harm. The transatlantic telegraph cables, an emblem of global communication, were made possible by the extraction and processing of gutta-percha, a natural latex found in Southeast Asian trees, which became a lucrative but exploitative industry for local workers. This highlights the interconnectedness of extraction and exploitation across time and space.

Submarine cables were seen as efficient and secure for communication and control over colonies during the colonial era, leading to the depletion of Palaquium gutta trees for their use in cable production. The gutta-percha ecological disaster of the Victorian era highlights the interdependence of technology, materials, labor practices, and environments. The historical roots of algorithmic computation in managing war, population, and climate change were intertwined with race science. Contemporary large-scale AI systems continue to drive environmental, data, and human extraction, posing new ecological threats.

Statistics were believed to be the first step in investigating the effect of selective processes on a race's characteristics, and these techniques became dominant tools in interpreting social and state information. Algorithmic computing and AI were developed to address social and environmental challenges but have been used to intensify industrial extraction and exploitation, depleting environmental resources. The tech industry's environmental policies and sustainability initiatives are highly produced and conceal the gargantuan amount of energy required to run computational infrastructures and the carbon footprint of AI systems.

The increasing use of cloud platforms is leading to significant environmental damage due to the high energy consumption required to power them, with estimates suggesting that the tech sector could contribute 14% of global greenhouse emissions by 2040. The energy demands of data centers are also expected to increase significantly in the coming years. AI models require large amounts of computational power, contributing to their high energy consumption and carbon footprint, with some estimates suggesting that running a single natural language processing model can produce the equivalent emissions of five gas-powered cars or 125 round-trip flights from New York to Beijing.

The energy consumption and carbon footprint of AI models used in the tech industry, particularly in natural language processing, are significant and growing. The computational technique of brute-force testing, driven by the belief that bigger is better, has contributed to a steep increase in energy consumption. However, the exact amount of energy consumption produced by AI models is unknown, as it is kept as a highly guarded corporate secret. The data economy is premised on maintaining environmental ignorance, and the tendency toward compute maximalism has profound ecological impacts. Some corporations are responding to growing alarm about energy consumption by claiming to be carbon neutral or promising to use renewable energy.

Major technology companies such as Microsoft, Google, and Amazon have pledged to become carbon negative by 2030 but continue to license their AI platforms, engineering workforces, and infrastructures to fossil fuel companies, contributing to the industry's environmental impact. China's data center industry draws 73 percent of its power from coal, emitting about 99 million tons of CO2 in 2018, and its electricity consumption from data centers is expected to increase by two-thirds by 2023. The global impact of resource extraction, including water use, goes beyond national boundaries. The National Security Agency's data center in Utah symbolizes the next era of government. Greenpeace calls on China's leading tech companies to scale up clean energy procurement and disclose energy use data.

The National Security Agency's data center in Utah sparked controversy due to its estimated daily water consumption of 1.7 million gallons in a drought-parched region. The NSA refused to share usage data and claimed national security reasons. Antisurveillance activists sought legal controls over water usage to shut down the facility, but the city of Bluffdale made a multiyear deal with the NSA for economic growth. The geopolitics of water intertwine with data centers, computation, and power, affecting the environment and climate in ways not fully recognized. The cloud is material and of the earth.

The growth of AI requires expanding resources and logistics, including the movement of minerals, fuel, hardware, workers, and consumer devices around the world through a global logistical machine. This machine is made possible by the standardized cargo container, which enables the modern shipping industry and models the planet as a single massive factory. The capacity of container ships in seaborne trade reached nearly 250 million deadweight tons of cargo in 2017, dominated by giant shipping companies. However, the cheap shipping costs disguise larger external costs.

The shipping industry, which is essential for the logistics of AI infrastructure, is largely overlooked by popular culture and media, resulting in "sea blindness." The industry produces 3.1% of global CO2 emissions, and most companies use low-grade fuel, leading to increased airborne sulfur and other toxins. Container ships also contribute to ocean pollution and cause harm to workers who spend long periods at sea. The rapid growth of cloud-based computation has driven an expansion of resource extraction, contradicting its portrayal as environmentally friendly. Understanding the hidden costs and wider systems involved in the shift towards automation requires factoring in these impacts.

Artificial intelligence (AI) is a kind of megamachine that relies on industrial infrastructures, supply chains, and human labor kept opaque. AI is metamorphic, relying on physical work, data centers, personal devices, transmission signals, datasets, and continual computational cycles. It is fundamentally intertwined with production, manufacturing, and logistics, and comes at a cost. The concept of the megamachine was developed in the late 1960s to illustrate how all systems consist of the work of many individual human actors.

The mines that drive AI are not only located in discrete locations but are diffuse and scattered across the earth, forming a planetary mine. Understanding the deep material and human roots of AI systems is crucial amid the impacts of anthropogenic climate change, but the scale and complexity of the AI system chain make it difficult to see the ongoing costs. Instead of making these complex assemblages transparent, connecting across multiple systems to understand how they work in relation to each other can lead to greater justice. The aim is to place the environmental and labor costs of AI in context with the practices of extraction and classification braided throughout everyday life.

A mining company purchased all available plots near Silver Peak, built a town called Blair and a large cyanide mill. The town thrived for a short time but the cyanide began to poison the ground and the gold and silver seams dried up, causing Blair to be deserted by 1918. The ruins remain as a reminder of the town's past. The current draw on the nearby lithium mine is aggressive and no one knows how long it will last, potentially causing Silver Peak to become a ghost town again in the near future.

The Amazon fulfillment center in Robbinsville, New Jersey is a large warehouse designed to accelerate the delivery of packages. Workers are monitored at all times, with every second of work being tallied. The center features robots that move heavy shelving units laden with products and conveyor belts that act as the factory's arteries.

Amazon's robotics technology is well tended, while the workers in the factory suffer from physical injuries due to the anxiety of meeting their picking rate. Humans are necessary for completing the tasks that robots cannot, but they are not the most valuable component of Amazon's machine. The basic unit of measurement at Amazon is the brown cardboard box, and workers must exit through metal detectors as an antitheft measure.

At Amazon, the cardboard box has a time to live driven by customers' shipping demands, and an algorithm called "the matrix" automatically assigns a new type of box based on data points to prevent breakages, saving time and increasing profits. Workers are forced to continually adapt to the control over time, and many corporations are investing in automated systems to extract more labor from fewer workers. The hybrid human-robotic distribution warehouses of Amazon highlight the trade-offs being made in the commitment to automated efficiency, and the shift in the experience of work towards increased surveillance, algorithmic assessment, and modulation of time.

The article discusses the ways in which humans are increasingly treated like robots in the context of artificial intelligence and how this affects the role of labor. It argues that large-scale computation is built on the exploitation of human bodies, and that to understand the future of work in the context of AI, we need to understand the past and present experience of workers. The article maps the geographies of labor past and present and shows how AI is built on the very human efforts of crowdwork and the privatization of time. It concludes that the lineage of the mechanized factory values increased conformity, standardization, and interoperability for products, processes, and humans alike, and that workplace automation is already a long-established experience of contemporary work.

The article discusses how white-collar employees are increasingly subjected to workplace surveillance, process automation, and a collapse between work and leisure time due to the expansion of AI systems. The author argues that this is a return to older practices of industrial labor exploitation and a continuation of the historical dynamics inherent in industrial capitalism. The expansion of labor automation continues to transfer more value to employers, and workers are rarely allowed to opt out of forced engagement with algorithmic systems. The article concludes that the encroachment of AI into the workplace represents new refrains on an old theme.

The history of workplace AI can be traced back to the Industrial Revolution, where automation of manufacturing tasks led to increased productivity and mechanization. The combination of mechanization and energy derived from fossil fuels resulted in a massive increase in production, but also transformed the role of labor in the workplace. Initially intended to assist workers, factory machines became the center of productive activity, shaping the speed and character of work. Workers adapted to the needs of the machine, and automation abstracted labor from the production of finished objects.

During the Industrial Revolution, the integration of workers' bodies with machines allowed factory owners to view their employees as a raw material to be managed and controlled. They used political power and force to restrict workers' movement and time, resulting in new divisions of labor, oversight, clocks, fines, and time sheets that influenced people's experience of time. The use of time became both a moral and economic issue, with workers pushing back and advocating for reducing the working day. Maintaining an efficient workforce required new systems of surveillance and control, including the inspection house, which placed all workers within sight of their supervisors.

Samuel Bentham, an English naval engineer employed by Prince Potemkin in Russia, developed an arrangement in the 1780s that allowed expert supervisors to monitor untrained subordinates. This arrangement inspired the panopticon, a design for a model prison featuring a central watchtower, by Bentham's brother, the utilitarian philosopher Jeremy Bentham. The panopticon began as a workplace mechanism before it was conceptualized for prisons. The inspection house was part of a strategy to modernize rural Russia and transform the peasantry into a modern manufacturing workforce.

The practice of observation and control in the workplace has a history that includes the Potemkin villages and the oversight of forced labor on colonial slave plantations. Today, the managerial class uses surveillance technologies such as tracking apps, social media analysis, and AI to monitor employees and predict their success. These practices raise concerns about privacy and autonomy in the workplace.

Artificial intelligence (AI) systems rely heavily on underpaid workers who perform various forms of labor, including supply chain work, on-demand crowdwork, and traditional service-industry jobs. This hidden labor is crucial to sustaining AI systems but is often poorly compensated. Workers who perform repetitive digital tasks like labeling training data and reviewing content rarely receive credit for their work, and a substantial number of them earn below their local minimum wage. The expansion of AI systems with new predictive capacities is leading to increasingly invasive mechanisms of worker management, asset control, and value extraction.

AI systems heavily rely on underpaid workers who perform various forms of labor, including tagging images and testing algorithms, and are commonly paid far below the minimum wage. The technical AI research community depends on cheap, crowd-sourced labor for tasks that cannot be done by machines, but there has been little debate about the ethical questions raised by this dependency. Clients using these platforms expect cheap, frictionless completion of work without oversight, treating human employees as little more than machines. In some cases, workers are asked to pretend to be an AI system, as with the digital personal assistant start-up x.ai's AI agent, Amy.

Some AI systems, including x.ai and Facebook's personal assistant M, rely heavily on human labor to sustain the illusion of being fully automated. This is known as Potemkin AI or fauxtomation. Until there is a way to create large-scale AI without extensive human labor, this will be a core logic of how AI works. Fauxtomation can also be seen in self-service kiosks and online systems that appear automated but actually relocate data-entry labor to customers or unpaid workers.

Many automated systems rely on underpaid digital pieceworkers and consumers taking on unpaid tasks to make them function, while companies seek to convince investors and the public that intelligent machines are doing the work. This approach can scale and produce cost reductions and profit increases while obscuring the reliance on remote workers being paid subsistence wages. Fauxtomation increases the disconnection between labor and value, making workers more easily exploited by their employers. Fauxtomation laborers face the fact that their labor is interchangeable, and they could be replaced at any time.

In the 18th century, a mechanical man called the Mechanical Turk played chess against humans and won, but it was later revealed that a human chess master was hiding inside the machine. Today, Amazon's micropayment-based crowdsourcing platform, also called Mechanical Turk, connects businesses with anonymous workers who perform microtasks to fill gaps in the company's AI systems. This allows humans to emulate and improve on AI processes, but it has been criticized for perpetuating the illusion that AI systems are autonomous and magically intelligent.

The concept of Potemkin AI involves the illusion of artificial intelligence being autonomous and self-sufficient, when in reality, it relies heavily on human labor. Examples include self-driving cars with human operators and web-based chat interfaces with human operators behind the scenes. Even activities like reCAPTCHA are used to train image recognition algorithms for free. The myth of AI being affordable and efficient relies on layers of exploitation, including mass unpaid labor to fine-tune AI systems. The reality is that contemporary AI depends on the exploitation of human labor across the entire supply chain.

Charles Babbage developed the Difference Engine and Analytical Engine, mechanical calculating machines for generating tables quickly. He believed industrial corporations were like computational systems with specialized units performing tasks to produce work. Babbage envisioned using computation, surveillance, and labor discipline to enforce efficiency and quality control. Babbage's economic thought diverged from Adam Smith's in that he believed the value of an object in a factory came from investment in design rather than the cost of labor. Babbage's ideas have become possible at scale with the recent adoption of AI in the workplace.

Charles Babbage viewed labor as a problem that needed to be contained by automation, and valued the manufacturing process over the labor force. He idealized machinery to maximize financial returns to plant owners, with little consideration for the human costs of automation. Today's workplace AI proponents prioritize efficiency, cost-cutting, and profits over assisting employees with repetitive work. This outcome is a necessary result of the standard business model of for-profit companies where the highest responsibility is to shareholder value. The system values the manufacturing process rather than the labor force.

Automation has led to the creation of insecure jobs with longer working hours and less pay. The Chicago meat-packing industry was among the first to implement mechanized production lines, which commoditized low-skill labor and led to poor working conditions. The Meat Inspection Act was passed in 1906 as a result of public outcry over food safety, but the focus on workers was lost.

Critique of exploitative labor dynamics is often met with superficial regulation, leaving underlying logics of production intact. Frederick Winslow Taylor's Principles of Scientific Management established a system for workplace management that focused on quantifying workers' movements for maximum efficiency at minimal cost. This exemplified Marx's description of the domination of clock time, reducing workers to mere carcasses. The example of Foxconn, which produces Apple products, illustrates this dynamic.

The text discusses how companies control and manage their employees, particularly in industries such as manufacturing and fast food. Foxconn's notorious management protocols and the use of time management to control workers are highlighted. In fast-food industries, time is measured down to the second and algorithmic scheduling systems determine workers' shifts, resulting in work schedules that can vary from day to day. The text also mentions a class action lawsuit against McDonald's for wage theft due to delaying clocking in and reducing staff based on algorithmic predictions.

The text discusses how algorithmic scheduling systems used by fast-food industries lead to time theft, reducing the predictability of employees' schedules and affecting their personal lives. The standardization and reduction of individual craft were central to Ray Kroc's management method, and Fordist factories scrutinized cycle time with supervisors walking the length of the factory with stopwatches. Now, employers passively surveil their workforce through electronic time clocks and timing devices that indicate the minutes or seconds left to perform tasks before notifying managers.

Companies such as WeWork and Dominos Pizza are using surveillance devices to track employees' behavior and performance. The data collected is used for algorithmic scheduling or sold to data brokers. Silicon Valley's demographic makeup and work culture, which glorifies workaholism and round-the-clock hours, contribute to the creation of such tools. This produces a vision of a standard worker that is narrow, masculinized, and reliant on unpaid or underpaid care work of others.

Technological forms of workplace management have made time tracking increasingly granular, with synchronization protocols like NTP and PTP establishing a hierarchy of clocks across a network. The master-slave metaphor, which has its roots in racist discourse, has been used throughout engineering and computation, including in early computing systems like the Dartmouth timesharing system. The problematic implication that control is equivalent to intelligence has continued to shape the AI field for decades, and the terminology has been seen as offensive by many and removed from some coding languages like Python.

Google's Spanner is a globally distributed, synchronously replicated database that supports various services including Google search, advertising, and Gmail. To synchronize time across millions of servers in hundreds of data centers, Google created a new distributed time protocol called TrueTime. TrueTime establishes trust relationships between local clocks of data centers, uses GPS receivers and atomic clocks, and allows a distributed set of servers to guarantee that events occur in a determinate sequence across a wide area network. TrueTime manages uncertainty caused by clock drift on individual servers by slowing down Spanner. This embodies the fantasy of bringing the planet under a single proprietary time code.

Google's TrueTime is a form of universal time that allows for the creation of a shifting timescale under the control of a centralized master clock. Proprietary forms of time have been used in the past to make machines run smoothly, such as railroad magnates who had their own form of time. The telegraph was a unifying technology that allowed for the compilation of time into a coherent grid and erasing more local forms of timekeeping, and it was dominated by the first great industrial monopoly, Western Union. The telegraph also enabled a new form of monopoly capitalism.

The telegraph and transatlantic cable allowed imperial powers to maintain centralized control over their colonies and made time a central focus for commerce, shifting from space to time. The infrastructural ordering of time creates centralizing power that determines new logics of information at a planetary level. Defiance of centralized time is seen in historical examples such as workers sabotaging factory time clocks and adding friction to the work process to reduce the value of time as a currency.

Controlling time is a strategy for centralizing power, from fine modulations of time in factories to big modulations at the scale of planetary computation networks. Artificial intelligence systems and video monitoring have allowed for greater exploitation of distributed labor around the world. The tech sector is creating a smooth global terrain of time to speed its business objectives. Workers have found ways to resist even when technological developments increase surveillance and company control. Amazon controls what the public can see in its fulfillment centers, but signs of unhappiness and dysfunction are harder to manage.

The article discusses the working conditions at Amazon's fulfillment centers and the conflicting perspectives on unionization. While Amazon claims to address worker complaints and insists unions are unnecessary, workers and advocates argue for better treatment, citing issues such as high productivity metrics, arbitrary scheduling changes, and inadequate communication from management. The article also highlights the experiences of workers from Minnesota's East African populations who were recruited by Amazon with sweeteners but faced unsustainable and inhumane working conditions.

The article describes the increasing protests and walkouts by Amazon workers across the United States, demanding better working conditions and higher wages. While Amazon representatives were willing to discuss some issues, they refused to budge on the productivity rate, which they claimed was their business model. Despite the challenges, workers' groups continued to organize and advocate for change, with some success in bringing attention to their concerns.

Workers and organizers at Amazon are building a movement to address issues of power and centralization represented by the relentless rhythm of the rate, which is threatened when the local warehouses are out of sync. This fight for time sovereignty is a long historical development that now includes workers in different industries, and cross-sector solidarity in labor organizing is nothing new. Many movements, such as those led by traditional labor unions, have connected workers in different industries to win victories for labor rights and protections.

Tech-related protests have led to the slogan "We are all tech workers," which highlights the wide labor force used to make tech products and the fact that many workers use technology for their jobs. However, there are risks in centering tech workers in long-standing labor struggles, and the goal of producing more just conditions for every worker should not depend on expanding the definition of tech work. A collection of mug shots of a woman across multiple arrests over many years shows the impact of the criminal justice system on individuals over time.

The National Institute of Standards and Technology (NIST) maintains a Special Database for facial recognition testing and has been involved in biometric data collection and analysis for over fifty years, working with the Federal Bureau of Investigation (FBI). After the 9/11 attacks, NIST was involved in creating biometric standards to track people entering the US. NIST's purpose is to make systems interoperable through defining and supporting standards, including developing standards for artificial intelligence. Mug shot images are included in NIST's extensive biometric collections.

The National Institute of Standards and Technology (NIST) includes thousands of mug shots of deceased people with multiple arrests in its Special Dataset 32, which is used as the technical baseline to test facial recognition software. The mug shots are presented as data points without context or names and were originally created to identify repeat offenders. Mug shots have been used as training data to fine-tune automated facial recognition, reducing nature to its geometrical essence.

The NIST Multiple Encounter Dataset contains standardized images of faces used to test the accuracy and speed of facial recognition algorithms. These images are used without the consent of the individuals depicted and are primarily viewed as technical resources. The dataset contains individuals who show signs of violence, but their images are used without regard for their personal histories. The mug shot databases are concerning due to the lack of privacy and the potential for misuse.

The NIST databases reflect a broader trend in the tech industry that considers everything as data and available for use, including mug shots of suspects and prisoners without regard for their personal histories or contexts. This represents a shift from image to infrastructure, where the meanings and ethical weight of images are presumed to be erased when they become part of a broader system. However, these images are anything but neutral and represent personal histories and structural inequities that can influence the performance of machine learning tools.

Computer vision systems lack the ability to understand social and historical contexts, and instead prioritize aggregate data over individual instances. This dehumanizes individuals, reducing them to mere data points. The collection of mug shots is used to train facial recognition systems, which are then used to monitor and detain more people. The capture of digital material has become fundamental to the AI field, and questions remain about how data is acquired, understood, and used in machine learning, and how it limits AI interpretation of the world.

The AI industry prioritizes ruthless pragmatism over context, caution, and consent-driven data practices, promoting the mass harvesting of data to create profitable computational intelligence. This transformation dehumanizes all forms of image, text, sound, and video as raw data for AI systems. The logic of extraction that has shaped the relationship to the earth and human labor is also a defining feature of how data is used and understood in AI. Massive amounts of data are required for machine learning systems, such as computer vision, to detect and interpret images, but the complexity of interpreting images is rarely acknowledged. It is common practice to scrape millions of images from the internet and use them to create a foundation for computer vision systems.

Machine learning algorithms rely on large training datasets to produce accurate predictions. The datasets are considered ground truth, which may not represent a factual reality. Supervised machine learning involves learners and classifiers that analyze relationships between inputs and desired outputs. Different machine learning models are used for specific tasks, such as facial recognition or sentiment analysis. To build a machine learning system, a developer must collect, label, and train a neural network on thousands of labeled images. However, biased training data can lead to inaccurate predictions.

Training datasets are essential for most machine learning systems to make predictions and assess performance. They serve as benchmarks for competitions like the ImageNet Challenge and are adapted and built upon over time, inheriting logic from earlier examples. Training data shapes the epistemic boundaries of AI and can lead to biased or inaccurate predictions if not carefully curated.

The limits of AI are created by training data, which is a brittle form of ground truth that cannot capture the complexity of the world. In 1945, Vannevar Bush predicted the need for enormous amounts of data for advanced machines to perform calculations. The relationship between data and processing machinery was imagined as one of endless consumption, with data seen as input for intelligible records rather than as an important aspect of crafting data and making systems work.

In the 1970s, AI researchers used expert systems, but it was impractical in real-world settings. By the mid-1980s, probabilistic or brute force approaches were used to find the optimal result, such as in speech recognition at IBM Research. They used statistical methods that required an enormous amount of training data. This resulted in a radical reduction of speech to data that could be modeled and interpreted without linguistic knowledge or understanding.

The reliance on data over linguistic principles in the development of statistical models presented challenges, with larger datasets being central to IBM's approach to improving probability estimates and capturing rarely-occurring outcomes. Finding large amounts of computer-readable text was difficult, leading the IBM Continuous Speech Recognition group to search for data in unlikely sources, ultimately finding it in the proceedings of a federal antitrust lawsuit against IBM.

IBM created a corpus of a hundred million words by digitizing deposition transcripts from a federal antitrust lawsuit, while the Penn Treebank Project collected four and a half million words of American English from various sources. The Enron fraud investigations also created a linguistic mine of over half a million email exchanges. However, the reliance on training data and the borrowing of text collections created issues and biases, and the resulting corpora were often not closely examined.

Language models and text corpuses have transformed natural language processing and machine learning, but they have also created problems. Text archives are not neutral collections of language, and language is not interchangeable. Skews, gaps, and biases in the collected text are built into the bigger system, and language models based on the wrong data can be problematic. Language collections are accounts of time, place, culture, and politics. There are no standardized practices to note where data came from or what biases it contains, which can influence all systems that rely on it.

The Face Recognition Technology (FERET) program, funded by the Department of Defense, developed automatic facial recognition for intelligence and law enforcement by creating a training set of portraits of over a thousand people in multiple poses. FERET became a standard benchmark for detecting faces and was used for monitoring airports, border crossings, and searching drivers license databases for fraud detection. FERET's primary testing scenarios involved identifying known individuals from a large population of unknown people using machine-readable, high-resolution, formal portrait-style photographs.

The FERET database, a collection of facial images used for facial recognition research, was created in the early 1990s with subjects who gave full consent, but lacked diversity. It became a benchmark for facial recognition technology, which rapidly expanded after 9/11 with increased funding. The internet's abundance of images became a valuable resource for AI research, leading to the creation of ImageNet and a shift towards more automated image analysis.

The rise of social media platforms provided an abundance of user-generated data in the form of images and text, which became a valuable resource for AI training sets. Tech industry giants were able to accumulate vast amounts of data through user uploads and unpaid labor for labeling, resulting in proprietary collections. Academia sought to replicate this advantage by combining internet-extracted images and text with the labor of low-paid crowdworkers.

ImageNet is a large dataset for object recognition that was conceptualized in 2006. The ImageNet team extracted millions of images from the internet to create a resource for providing critical training and benchmarking data for object and image recognition algorithms. The dataset grew enormous with over 14 million images organized into more than 20,000 categories. However, ethical concerns about taking people's data were not mentioned in any of the team's research papers, and the labeling of the images was a major concern.

The ImageNet project hired undergraduate students to manually find and label images, but realized it would take too long and too much money. They then turned to Amazon Mechanical Turk, becoming the largest academic user of the platform, and utilized crowdsourced labor to label thousands of categories for object recognition. However, the labeling included cruel, offensive, and racist categories, which became standard practice for mass data extraction without consent. These practices eventually came back to haunt the project, marking a shift away from consent-driven data collection in the early years of the 21st century.



The practice of extracting photos from the internet for use in datasets without agreements, releases, and ethics reviews became prevalent in staged photo shoots. 

Some institutions took it to the extreme by secretly capturing photos of students and faculty without consent to train facial recognition systems. 

These practices were criticized, and one dataset was even removed from the internet after being used for the surveillance of ethnic minorities by the Chinese government. 

Similar incidents occurred at other universities, where images were extracted without consent and used to train automated imaging systems. 

Microsoft's MS-Celeb dataset, which scraped photos of celebrities without their consent, is another example.



The MS-Celeb dataset, which scraped photos of famous and non-famous individuals without consent, was the largest public facial recognition dataset in the world. 

Even when datasets are anonymized and released with caution, people have been reidentified or sensitive details about them revealed. 

Such datasets can lead to individual and group privacy harms, as in the case of the New York City taxi dataset, which revealed the religious beliefs of some drivers. 

Despite these ethical and political concerns, the AI field continues to collect large datasets for machine learning purposes.

The belief that data is valuable and should be collected in large quantities is a common myth that has been manufactured and supported over time. This belief is driven by economic incentives and the potential for profit, and is perpetuated by institutions and technology. The moral imperative to collect data has led to the normalization of mass data extraction, despite potential epistemological problems and harms.

The belief that more data is better has led to the normalization of mass data extraction, despite potential negative impacts. The idea of what counts as data is determined by norms and standards of each discipline and institution. Data is often described as a resource or investment, with the metaphor of data as oil being common. However, this metaphor disguises the material origins and ends of data and justifies its extraction. Viewing data as capital is in line with neoliberal visions of markets. These metaphors have been used for centuries by colonial powers to justify extraction from primitive and unrefined sources.

The use of digital traces and scoring metrics to rank human activities functions as a way to extract value, with those who have the right data signals gaining advantages. Data operates as a form of capital, justifying a cycle of ever-increasing data extraction and datafication of all spaces. Mass data extraction is the new frontier of accumulation and the foundational layer that makes AI function. Machine learning models require ongoing flows of data, justifying more extraction from as many people as possible to fuel the refineries of AI.

The emergence of data subjects in machine learning has shifted ethics away from human subjects, leading to minimal ethical review for publicly available datasets. The fields of applied mathematics, statistics, and computer science had not historically been considered forms of research on human subjects. Ethics protections like university-based institutional review boards had accepted this position for years, leading to a strong presumption that publicly available datasets pose minimal risks and should be exempt from ethics review. The potential harms of AI in real-world situations have expanded, affecting entire communities as well as individuals.

The assumptions about data collection in the past are out of step with the current machine learning landscape, where datasets are easily connectable and continuously updatable. However, the risk profile of AI is changing as its tools become more invasive, and researchers access data without interacting with their subjects. Researchers use databases like CalGang, which is known to have major inaccuracies, to train predictive AI systems, leading to the disproportionate inclusion of Black and Latinx people.

Researchers using biased data to train predictive AI systems raises ethical questions about the responsibility for harm caused. Separating ethical concerns from technical considerations perpetuates the idea that scientific research has no responsibility for the ideas it propagates. The reproduction of harmful ideas in AI is particularly dangerous as it moves from the laboratory to production systems.

The use of machine learning and data science methods can create a disconnect between researchers and the individuals and communities at risk of harm. This detachment has been a long-established practice in the field of AI, which seeks to circumvent human contexts. There is a culture of data harvesting that can be exploitative and invasive, and can produce lasting harm. There are strong incentives to maintain this colonizing attitude towards data, and this culture continues to grow despite concerns. Joseph Weizenbaum argued that scientists and technologists should actively strive to reduce psychological distances and think deeply about the consequences of their work.

AI systems are being built using thousands of freely available datasets that capture biodata, including forensic, biometric, sociometric, and psychometric data. Many of these datasets were harvested without people's knowledge or consent from online sources and government agencies. The use of this data raises ethical, methodological, and epistemological questions and has extended into new areas of human life, such as voice data from devices, physical data from wearables, and data on reading habits. The collection of people's data raises clear privacy concerns, such as the deal made between Britain's National Health Service and Google's subsidiary, DeepMind, to share patient data records.

The article discusses how the practices of data extraction and construction of training datasets have led to the privatization of knowledge value from public goods, which is a form of erosion of the commons. The new AI gold rush has led to an expansionist logic of never-ending data collection, which has enriched tech companies with the largest data pipelines and diminished spaces free from data collection. This extractive logic has become a core feature of how the AI field works, impacting the public good.

The article argues that data collection for AI is a social and political intervention that shapes how artificial intelligence works in the world and which communities are most affected. The way data is understood, captured, classified, and named has enormous ramifications for the use of AI. The myth of data collection as a benevolent practice in computer science obscures its operations of power and protects those who profit most while avoiding responsibility for its consequences. The author also provides an example of how data was collected and labeled in the past by discussing Samuel Morton's collection of human skulls.

The article discusses how craniometry, a method of measuring the size and shape of skulls, was used to classify and rank human races objectively based on physical characteristics. This was a reflection of the colonialist mentality of the time, which legitimized the belief that distinct human races had evolved separately at different times (polygenism). Morton, a prominent craniometrist, claimed that white people had the largest skulls and that black people were on the bottom of the scale. His tables of average skull volume by race were regarded as cutting edge science and used to maintain the legitimacy of slavery and racial segregation in the United States, even after his studies were no longer cited. However, Stephen Jay Gould argues that Morton's work was not the objective evidence it claimed to be.

The article discusses how Stephen Jay Gould and others reexamined Morton's evidence on craniometry and found errors and procedural omissions that suggest prior prejudice influenced his findings of white supremacy. Craniometry was a leading numerical science of biological determinism in the 19th century that was based on flawed assumptions about brain size and race. This race science supported white supremacy and made specific political ideas about race possible while closing down others, even after it was debunked. Gould's case highlights the social context of science and the potential for unconscious finagling and self-delusion to shape supposedly objective data.



 Legacy of correlating cranial morphology with intelligence and claims to legal rights has epistemological problems with measurement and classification in AI. 

 The underlying worldview behind racist models of intelligence should be condemned instead of calling for more accurate or fair skull measurements. 

 Morton's invalid assumptions about intelligence, race, and biology had far-ranging social and economic effects. 

 The politics of classification is a core practice in AI, informing how machine intelligence is recognized and produced. 

 AI systems often produce discriminatory results along the categories of race, class, gender, disability, or age, leading to pressure to reform tools or diversify data. 

 More fundamental questions about how classification functions in machine learning and its interaction with the classified are often missing. 

 The social and political theories underlying classifications of the world need to be examined.



 Classifications are powerful technologies that can become invisible in working infrastructures. 

 Classification is an act of power that can shape the social and material world. 

 The focus on bias in AI has drawn attention away from the core practices of classification and their politics. 

 AI uses classification to encode power and naturalize hierarchies, magnifying inequalities. 

 Discriminatory AI systems are legion, with examples of gender, race, and age bias. 

 Scholars have shown how AI perpetuates and even amplifies existing biases and inequalities.

Instances of AI bias and discrimination frequently occur and are often addressed through technical interventions or the introduction of new systems. However, there is a lack of public debate about the root causes of these issues. An example of bias in action is the case of Amazon's AI system designed to recommend and hire workers, which was trained on a dataset of employee resumes and began to assign less importance to commonly used engineering terms, favoring more subtle cues instead.

Amazon experimented with an AI system for hiring that was biased against women, downgrading resumes from candidates who attended women's colleges and including the word "women." Even after editing the system to remove explicit references to gender, biases remained due to the gendered use of language itself. This revealed the ways in which bias already exists in language and in the industry as a whole. The AI industry has traditionally focused on fixing bias as a bug rather than a feature of classification, creating its own problems.

The article discusses the problem of bias in artificial intelligence systems and the limitations of attempts to fix it. It suggests that instead of only looking at biased datasets, one should examine the mechanics of knowledge construction itself, which involves observing how patterns of inequality shape access to resources and opportunities, which in turn shape data. The article highlights IBM's attempt to address bias by creating a more inclusive dataset, but notes that such efforts have limitations.

The article discusses IBM's attempt to address bias in facial recognition systems by creating a more diverse dataset. However, the classifications used in the dataset reveal the politics of what diversity meant in this context and depoliticized the idea of diversity. The practice of classification centralizes power and allows designers to decide which differences make a difference. The article also highlights the problematic claim that aspects of identity such as race and gender can be observed from the face and embedded in technical systems, which is an example of digital epidermalization.

The use of surveillance technology to produce a truth about the body and identity alienates the subject, and IBM's approach to classifying diversity is problematic because it relies on machine learning techniques that do not consider culture or heritage. The technical claims about accuracy and performance are often based on political choices about categories and norms. The act of classification has always been aligned with power, and bias has a historical lineage dating back to the fourteenth century.

Bias in machine learning refers to a systematic classification error that can occur during the process of generalization, and is contrasted with variance, which refers to an algorithm's sensitivity to differences in training data. The technical understanding of bias in machine learning draws from statistics, and it is one of many meanings of the term, which can also refer to preconceived notions or opinions in law or cognitive biases in psychology.

The use of the term "bias" in relation to AI systems is limited and there is a failure to address the deeper structural problems causing inequity. AI systems reflect and serve the incentives of a wider extractive economy, resulting in a persistent asymmetry of power. Every dataset used to train machine learning systems contains a worldview, and the process of creating a training set requires inherently political, cultural, and social choices. The case of ImageNet illustrates the power structures built into the architectures of AI world-building.

ImageNet's structure is based on the semantic structure of WordNet, which attempts to organize the entire English language into synonym sets. ImageNet's taxonomy is organized into a nested hierarchy derived from WordNet, moving from more general concepts to more specific ones, with each synset representing a distinct concept. The first indication of the strangeness of ImageNet's worldview is its nine top-level categories, and its structure is labyrinthine, vast, and filled with curiosities.

The ImageNet dataset categorizes images into a wide range of categories, including plants, geological formations, sports, artifacts, fungi, people, animals, and miscellaneous items. There are thousands of nested and specific classes within these categories. The images range from high-quality stock photography to blurry phone photos and include depictions of people, animals, and objects. Human classifications within ImageNet highlight the politics of classification, with only male and female bodies recognized as natural and hermaphrodites categorized under "sensualist" and "bisexual" categories.

The classification hierarchy in ImageNet reflects binary gender norms and naturalizes gender as a biological construct, rendering transgender and gender nonbinary individuals invisible or deviant. This practice of allocating people into gender or race categories without their input or consent has a long history and has been used to justify forms of violence and oppression. The looping effect shows how these classifications not only affect the people being classified but also change the classifications themselves.

The construction of classifications of people can stabilize political categories, and highly influential infrastructures and training datasets in AI contain political interventions within their taxonomies. The concept of an apple is more nouny than the concept of light, and ImageNet's logic flattens everything out and pins it to a label. ImageNet's subcategory with the most associated pictures under the top-level category Person was gal, followed by grandfather, dad, and chief executive officer. ImageNet contains a profusion of classificatory categories, including ones for race, age, and nationality, and this exercise is a profoundly ideological one.

ImageNet's taxonomy purports to classify photos of people with the logics of object recognition, but there are many problems with it. Categories for racial and national identities remain, and people are also labeled by careers or hobbies. ImageNet also contains nonvisual categories that describe a relationship. Offensive and harmful categories, including misogynistic, racist, ageist, and ableist ones, exist in ImageNet's Person categories. The list includes Bad Person, Call Girl, Deadeye, Drug Addict, Flop, Slut, and many more.

ImageNet, a dataset used for object recognition, contained offensive terms such as racist slurs and moral judgments in its Person categories for ten years. The ImageNet Roulette project, which allowed people to upload images to see how they were classified, brought media attention to the dataset's inclusion of racist and sexist terms. The creators of ImageNet published a paper titled "Toward Fairer Datasets" and removed 56% of the People categories and their associated images, deeming them unsafe. However, the focus on hateful categories avoids addressing larger questions about the dangers and complexities of human classification in the dataset.

ImageNet's categories are not neutral as they contain assumptions and stereotypes about race, gender, age, and ability. The dataset's classification logic is politically charged even when words are not offensive. Tech companies' proprietary classification systems are difficult to investigate and criticize. ImageNet's Person categories are harvested from image search engines without people's knowledge, and crowdworkers are paid low wages to label and package them. The labeled images in the dataset are riddled with stereotypes and errors.

The article discusses the limitations and problems of ImageNet's labeling system, which simplifies and compresses complex cultural materials into quantifiable entities. The focus on making training sets fairer by deleting offensive terms fails to address the power dynamics of classification and the underlying logics. The author argues that the worldview of ImageNet is typical of many AI training datasets that flatten complex social, cultural, political, and historical relations. This leads to false assumptions about gender, race, and sexuality being natural and fixed biological categories, which is insidious in the widespread efforts to classify people in technical systems.

The UTKFace dataset is an example of how AI training sets use dangerously reductive categorizations of gender and race. The dataset categorizes gender as a forced binary and race into five classes, echoing the problematic racial classifications of the 20th century. The South African apartheid system is used as a historical reference, where a crude racial classification scheme divided citizens and governed their lives, leading to restrictions on movement, forced removals from land, and forbidden interracial sexuality. The database for these classifications was designed and maintained by IBM.

The concept of pure race signifier has always been disputed and classification systems have caused harm. Machine learning systems reproduce systemic inequality by introducing classifying logics that reify existing racial categories or produce new ones. Characteristics such as criminality and gender are profoundly relational and context-dependent, and making predictions based on them is scientifically and ethically problematic. Machine learning systems construct race and gender and have long-lasting ramifications for people.

Classification systems erase technical frailties, priorities, and political processes of categorization, and create an outside of normalcy. Technical systems make political and normative interventions, restricting the range of how people are understood and can represent themselves. Classifying people is an imperial imperative, and classifications produce and limit ways of knowing. Redressing the power and politics in classification systems requires accounting for historical and ongoing injustices and creating systems that prioritize justice and equity.

The challenge for AI systems is how to represent society without perpetuating oppression and discrimination. Justice cannot be computed, and understanding the impact of AI on data, workers, the environment, and individuals is crucial. A new approach is needed that attends to the uneven distribution of advantage and suffering, and challenges normative assumptions about identity. Classificatory infrastructures are necessarily reductionist and proliferate in machine learning platforms, but there is a need for sensitivity to the topography of classification and the distribution of ambiguity.

Machine categorization and human interaction with it is creating unpredictable consequences, and each classification has its consequence. Historical harmful categorization did not fade away through scientific research and ethical critique alone, but through sustained protest and political organizing. Classificatory schemas enact and support power structures that formed them, and change requires considerable effort. Private technology companies operate massive engines of classification with little oversight or avenues for public contestation.

The article discusses the history of affect recognition in artificial intelligence and its problematic roots in the research of psychologist Paul Ekman. Ekman conducted experiments with the Fore people of Papua New Guinea to support his theory of universal emotions, but his methods and lack of cultural understanding have been criticized. The article argues that the hidden processes of AI and affect recognition require a collective political response.

The history of computer-based emotion detection, which is now used in various fields, has raised ethical concerns and scientific doubts. There is no reliable evidence that a person's emotional state can be accurately assessed by analyzing their face. Despite this, the idea that there is a small set of universal emotions readily interpreted from the face became widely accepted in the AI field. The theories behind affect recognition are connected to U.S. intelligence funding during the Cold War, foundational work in computer vision, and post-9/11 security programs. The work of Paul Ekman is one example that illustrates the complex forces driving the field.

Automated affect recognition, which aims to detect and classify emotions by analyzing any face, is being built into several facial recognition platforms. Despite a lack of scientific evidence that they work, these systems are already widely deployed, particularly in hiring. Emotion recognition is as compelling as it is lucrative for militaries, corporations, intelligence agencies, and police forces, but it can be a powerful agent in influencing behavior and training people to perform in recognizable ways.

Companies like HireVue, Affectiva, and Emotient use machine learning to detect emotions from facial cues in job interviews, advertising, and driving. Affectiva has built the world's largest emotion database, while Microsoft, Amazon, and IBM have also designed systems for emotion detection.

Emotion recognition systems rely on the assumption that there are universal emotional categories that can be detected through facial expressions. These systems were developed from the intersection of AI, military priorities, and psychology. Paul Ekman, a leading figure in this approach, was influenced by Silvan Tomkins' work on affect, which emphasized the universality of emotions. However, questions arise about the taxonomization of emotions and the assumptions underlying emotion recognition systems. Despite this, these theories are widely applied in AI emotion recognition.

Silvan Tomkins' theory of biologically-based universal affects provided a set of principles that could be applied everywhere and simplified complexity. Tomkins developed his theory to challenge behaviorism and psychoanalysis, which he believed reduced human consciousness. He argued that motivation was governed by affects and drives, with affects playing the most important role in human behavior. Tomkins' theory emphasized the inability of humans to recognize their own feelings, which led to later applications of affect theory in AI emotion recognition systems.

The complexity of human motivation and affective feelings makes it difficult to identify their precise causes and manage them effectively. Affects are not strictly instrumental and have a high degree of independence from stimuli and objects. However, Tomkins proposed that the primary affects are innately related to facial expressions, as evidenced by works such as Darwin's "The Expression of the Emotions in Man and Animals" and Duchenne de Boulogne's "Mécanisme de la physionomie humaine." Tomkins assumed that facial displays of affects were a human universal.

Tomkins believed that sets of muscle, vascular, and glandular responses in the face and body generate sensory feedback for affective displays, and that subcortical centers store specific programs for each affect. However, the interpretation of facial expressions depends on individual, social, and cultural factors, leading to different dialects of facial language in different societies. This cultural variability has implications for studying facial expression and training machine learning systems. Ekman was pushed by the Department of Defense to research cross-cultural nonverbal communication, despite not having experience in this area.

The text describes how Paul Ekman received funding from ARPA despite his lack of expertise and knowledge, due to the efforts of his colleague Hough, who needed to distribute his money quickly to avoid suspicion from Senator Frank Church. Ekman's research on facial expressions of emotion and cultural differences in gestures received a large grant from ARPA, which became the first of many agencies to fund his career and the field of affect recognition. Ekman's studies followed the design of Tomkins's methods, using photographs of posed facial expressions to test subjects drawn from different countries.

The text describes how Ekman's early studies to prove universality in facial expressions largely duplicated Tomkins's methods, using photographs of posed facial expressions to test subjects from different countries. However, this methodology had problems, as it alerted subjects to the connections between facial expressions and emotions and raised concerns about the validity of the results. Ekman found some cross-cultural agreements, but his findings were challenged by Birdwhistell, leading Ekman to study indigenous people in Papua New Guinea. He later showed U.S. research subjects a photograph and asked them to choose among six affect concepts to prove his theory of universal associations between facial behaviors and emotions.

The ancient Greeks used physiognomy to judge a person's character based on their physical appearance, which included racial classifications. During the 18th and 19th centuries, physiognomy became part of anatomical sciences and was blended with scientific knowledge by Lavater. He believed bone structure connected physical appearance and character type, and used skull measurements to support emerging nationalism, racism, and xenophobia. This work was elaborated on by phrenologists and in scientific criminology, leading to types of inferential classifications that recur in contemporary AI systems.

French neurologist Duchenne codified the use of photography and electrical shocks to study human facial expressions and interior mental or emotional states. His experiments on vulnerable subjects at the Salpetrière asylum connected physiognomy and phrenology with modern investigations into physiology and psychology. Duchenne believed that photography and technical systems would make representation objective and evidentiary, suitable for scientific study.

The article discusses the use of photography in the study of emotions and facial expressions. Duchenne used photography to categorize facial expressions, but his images were highly manipulated. Ekman also used photography to study microexpressions, but found slow motion photography necessary. He developed the Facial Action Scoring Technique (FAST) to detect and analyze facial expressions, but it ran into problems when scientists produced expressions not included in its typology. Ekman then identified 40 distinct muscular contractions on the face, called Action Units, as the basic components of facial expressions.

Ekman and Friesen published the Facial Action Coding System (FACS) in 1978, but it was labor-intensive to use. Ekman heard about a solution to automate measurement at a conference in the 1980s, likely Igor Aleksander's neural network system called WISARD, which was trained on a database of football hooligans. Early researchers in facial recognition found common cause with Ekman's approach. Ekman helped set up an informal competition between two teams working with FACS data, including Terry Sejnowski and Marian Bartlett, both of whom have since become prominent in the field of affective computing.

The development of affect recognition technology was influenced by two teams led by Emotient and the University of Pittsburgh, who used standardized datasets like the Cohn-Kanade dataset to create machine learning applications. The Facial Action Coding System (FACS), created by psychologist Paul Ekman, provided a stable set of labels for categorizing facial expressions and measurements for experimentation. The Department of Defense funded the FERET program to collect facial photographs, and machine learning researchers began assembling, labeling, and making public datasets. The CK dataset, which followed Ekman's tradition of posed facial expressions, allowed for the development of machine learning applications for affect recognition.

As the field of affect recognition grew, standardized photo databases were created, such as Karolinska Directed Emotional Faces and the Extended Cohn-Kanade (CK+) Dataset, which included both posed and spontaneous expressions. Affectiva emerged from MIT Media Lab, collecting naturalistic facial expressions in real-life settings through webcam recordings of individuals watching commercials, and hand-labeling the data using Ekmans FACS. However, issues of bias and privacy arose in collecting and using this data.



FACS, developed from Ekman's archive of posed photographs, became widely influential in lie detection software and computer vision. 

Ekman's ideas on facial expressions and deception detection were used by various clients, including the FBI, CIA, and Pixar. 

Ekman's techniques were sold to security agencies, leading to the creation of the SPOT program, which has been criticized for its lack of scientific methodology and potential for racial profiling.



Critiques of Ekman's theories grew as his fame increased, with skepticism emerging from multiple fields. 

Anthropologist Margaret Mead was an early critic of Ekman's belief in universal, biological determinants of behavior, emphasizing the role of cultural factors. 

Other scientists have since shown that fundamental questions about facial expressions and emotions remain unsolved and that AI technology using facial expression detection does not reliably indicate internal mental states.

The understanding of emotions, including their formulation, expression, and physiological functions, remains unsettled. Criticisms of Ekman's theory of emotions have been raised by historian Ruth Leys, who highlights the circularity of Ekman's method in assuming that facial expressions in photographs are universally recognized and free of cultural influence. The implementation of Ekman's ideas in technical systems also faces challenges, as datasets often rely on actors simulating emotional states, leading to AI systems being trained on constructed material rather than natural interior states.

Automating the connection between facial movements and discrete emotional categories is challenging, and the uniqueness of bodily responses to each type of emotion has little consistent evidence. Facial expressions may not reflect honest interior states, and despite decades of scientific controversy, Ekman's work on facial expressions continues to be cited in hundreds of papers without acknowledging the uncertainty. The affective computing researcher Arvid Kappas questions the idea of computers trying to sense emotions at all, and Lisa Feldman Barrett's research team in 2019 found evidence against Ekman's work.

A review of the literature on inferring emotions from facial expressions found that facial expressions are not reliable indicators of emotional states across cultures and contexts. The approach of reading emotions from the face has endured due to military research funding, policing priorities, and profit motives shaping the field. The team was critical of AI companies claiming to automate the inference of emotion and concluded that very little is known about how and why certain facial movements express instances of emotion.

Ekmans theories and methodologies are commonly cited in the AI field despite emotions not being easily classified or reliably detectable from facial expressions. The focus has been on increasing the accuracy rates of AI systems rather than addressing the bigger questions about emotions. AI systems for emotion detection are premised on the idea that emotions are biological categories imposed by nature, assuming emotional categories are givens, rather than emergent and relational. The article suggests questioning the origins of emotion categories and their social and political consequences instead of building more systems that group expressions into machine-readable categories.

A blogger used a polygraph system with facial and speech analytics from various AI tools to claim that Congresswoman Ilhan Abdullahi Omar was lying and registering high on stress, contempt, and nervousness. Conservative media outlets ran with the story, but the danger of affect recognition tools is that they take us back to the phrenological past and have racial biases. Emotions are complex and emotion detection systems do not directly measure peoples' interior mental states. The scientific foundations of automated emotion detection are in question, yet a new generation of affect tools is emerging.

Companies continue to seek out new sources to mine for facial imagery despite the unreliability of affect detection. The desire to automate affect recognition poses a risk of unfair judgments on job applicants, students, and customers based on their facial cues. The areas of life in which these systems operate are expanding, yet they rely on a narrow understanding of emotions and are based on questionable methodologies. This takes us back to the profound limitations of capturing the complexities of the world in a single classificatory schema.

The desire to oversimplify complex experiences for easy computation and market packaging results in AI systems unable to capture the nuances of emotional experiences. The Snowden archive contains classified documents on the use of machine learning in the intelligence sector by some of the wealthiest governments in the world, capturing the years when data collection became widespread.

The Snowden archive reveals the intelligence community's contribution to the development of techniques now known as AI, and a parallel secret AI sector with similar methods but different objectives. Documents outline programs such as TREASUREMAP, which aims to build a real-time interactive map of the internet, and FOXACID, which records everyday activity through a browser. The documents read like AI marketing brochures of today and reveal similarities to the work of social network mapping and manipulation companies.

The US intelligence agencies and military have been major drivers of AI research since the 1950s, with AI often guided by military support and priorities. The NSA has been working to pursue legal authorities and policy frameworks to fit the information age, and analysts deploy targeted emails requiring guilty knowledge about the target. The military priorities of command and control, automation, and treasure mapping have shaped AI research, with DARPA funding being the primary patron for the first twenty years of AI research.

DARPA funding has profoundly shaped the field of AI, with technical methods such as computer vision, automatic translation, and autonomous vehicles marking the field. Concepts of situational awareness and targeting have driven AI research, creating frameworks that inform both industry and academia. AI has expanded modes of information extraction and informed a social theory of tracking and understanding people through metadata, which became a vision of threat identification and assessment. The relationship between national militaries and the AI industry has expanded beyond security contexts to municipal arms of government and law enforcement agencies. State and corporate actors collaborate to produce infrastructural warfare.

The commercial surveillance sector is aggressively marketing its tools and platforms to police departments and public agencies. Algorithmic governance is part of and exceeds traditional state governance, and the AI industry is challenging and reshaping the role of states. AI systems operate within a complex interwoven network of multinational and multilateral tools, infrastructures, and labor. Hybrid systems are often created with infrastructure from different countries, and China has stated its commitment to be the global leader in AI. The data practices of China's leading tech companies are also a concern.

The language of "AI war" creates a nationalized race for technological superiority, asserting sovereign power over AI and redrawing the locus of power of transnational tech companies. Geopolitical competition blurs the lines between commercial and military sectors, incentivizing close collaboration and funding. The US has explicitly pursued national control and international dominance of AI to secure military and corporate advantage, as seen in the Third Offset strategy under Ash Carter. The strategy compensates for underlying military disadvantage by changing conditions.

The "Third Offset" is a military strategy that combines AI, computational warfare, and robots. The US military lacks the necessary resources and expertise, so it needs to partner with technology companies. However, there is political pushback after the Snowden disclosures and limitations were introduced on the NSA's access to real-time data from Silicon Valley. The possibility of a military-industrial complex around data and AI remains tantalizingly close, but the tech sector needs to be convinced without alienating their employees and deepening public opposition.

Project Maven was a program created by the Department of Defense to integrate artificial intelligence and machine learning more effectively across operations. Its goal was to create an AI system that could detect and track enemy combatants by analyzing military data collected from satellites and battlefield drones. The program needed technical platforms and machine learning skills centered in the commercial tech sector, so the Defense Department decided to pay tech companies for the analysis. Google won the first contract, which caused controversy within the company and sparked ethical concerns among employees.

Google's TensorFlow AI infrastructure was used by the Pentagon to detect objects and individuals in drone footage. Fei-Fei Li, an AI/ML expert at Google, insisted that the project remain secret. However, when Google employees discovered the project's use for warfare purposes, they protested and demanded that the contract be canceled. Google withdrew from the project and the competition for the JEDI contract, which went to Microsoft. Google later released its Artificial Intelligence Principles, which included a section on not pursuing technologies whose principal purpose is to cause injury to people.

The debate over using AI in warfare shifted from whether to use it at all to questions of precision and technical accuracy. However, the problems with automated warfare go beyond accuracy, as questions remain about who builds training sets and decides what constitutes an imminent threat. The AI industry is divided on the relationship between the military and civilian spheres, and the AI war instills fear and promotes unquestioning support for a nationalist agenda. Despite previous protests, Google's chief legal officer stated that the company is pursuing higher security certifications to work with the Defense Department.

Tech companies are aligning with the interests of the nation-state and their capacities are exceeding traditional state governance. The outsourcing of state functions to technology contractors has led to militarized forms of pattern detection and threat assessment being used in local government services. Palantir is an example of this phenomenon, founded by PayPal billionaire Peter Thiel, who argues that AI is first and foremost a military technology. Palantir describes itself as patriotic, producing military-styled tools to be provided to anyone willing to pay for it.

Palantir, a technology company, initially worked with federal military and intelligence agencies and later expanded to work with hedge funds, banks, and corporations. It was backed by In-Q-Tel, a venture capital firm funded by the CIA, and deployed similar approaches as seen in the Snowden documents to track and evaluate people and assets. Palantir became a preferred outsourced surveillance provider, including designing databases and management software for Immigration and Customs Enforcement (ICE). Its business model is based on data analysis and pattern detection using machine learning, combined with generic consulting.

Palantir, an intelligence platform designed for the global War on Terror, is now being used by various US government agencies to surveil and target ordinary Americans, including detecting Medicare fraud, criminal probes, screening air travelers, and keeping tabs on immigrants. Palantir's approach is reminiscent of the NSA, collecting everything and then looking for anomalies in the data. Its phone app, falcon, functions as a vast dragnet, gathering data from multiple law enforcement and public databases. Palantir's patent applications provide insight into the company's approach to AI for deportation.

Palantir's systems use facial recognition and back-end processing to photograph people in short-time-frame encounters, and their image is run against all available databases. These systems, initially designed for military intelligence infrastructures, are now being sold to local communities, including supermarket chains and law enforcement agencies. The shift from traditional policing to big data surveillance is transforming the process of surveillance entirely, with police turning into intelligence agents. This migration towards intelligence activities has the potential to result in widespread police reliance on algorithms with little oversight.

The use of Palantir's software for surveillance creates a reinforcing loop of logic that reproduces inequality, making poor, Black, and Latinx communities subject to even greater surveillance. Palantir's point system leads to biased policing and a feedback loop where those included in a criminal justice database are more likely to be surveilled, deepening inequity and justifying racially biased surveillance. The use of intelligence models in local policing exacerbates historical inequality and overpolicing.

Little attention has been given to whether private vendors of AI systems used by governments should be legally accountable for the harms produced. Currently, commercial algorithmic systems contribute to government decision-making without meaningful mechanisms of accountability. Legal scholars argue that developers of AI systems that directly influence government decisions should be found liable for harms in the same way as states can be. Contractors like Palantir and Vigilant Solutions have little incentive to ensure that their systems do not reinforce historical harms or create new ones. Vigilant Solutions turns surveillance tools into a thriving private enterprise outside constitutional privacy limits.

Vigilant, a California-based company, stores license plate images in a massive database and sells access to it to police, private investigators, banks, insurance companies, and others. They partner with governments to provide ALPR systems to use on patrol, and in return, receive records of outstanding arrest warrants and overdue court fees. Vigilant also signed a significant contract with ICE, giving the agency access to five billion records of license plates gathered by private businesses and local law enforcement agencies. ICE buys access to Vigilant's systems, which violates state data-sharing laws and their own privacy policy.

Vigilant, a company that stores license plate images, now includes facial recognition technology in its crime-solving toolkit. This blurs the line between private contractors and state entities, creating opaque forms of data harvesting without regulation or accountability. Social media crime-reporting apps like Neighbors, Citizen, and Nextdoor allow users to get alerts, discuss incidents, and share security camera footage, often with police. Amazon's Neighbors app, which relies on its Ring doorbell cameras, classifies footage into categories and builds large-scale training datasets, with classificatory logics aligned with the battlefield logics of allies and enemies. There are concerns about privacy violations and racist commentary.

Amazon's Ring surveillance system disproportionately portrays people of color as potential thieves and is also used to monitor and report Amazon employees. The system has been marketed aggressively to police departments, with partnerships negotiated with over 600 departments. Police are incentivized to promote the Neighbors app, which encourages people to download Ring and creates a self-perpetuating surveillance network. The use of such apps means individuals are contributing to the state's security apparatus, and targeting should be considered as one interconnected system of power.

Intelligence agencies have dispersed throughout social systems and are promoted by tech companies in commercial and military AI sectors. The military logic of targeting is based on the signature, which involves using metadata to kill individuals believed to be terrorists. Drone strikes are sold as precise, but are only as accurate as the intelligence that feeds them. Signature strikes are not about precision, but correlation.

IBM developed a machine learning platform to detect refugees who might be connected to jihadism during the 2015 Syrian refugee crisis. The program aimed to automatically distinguish a terrorist from a refugee using data extraction and social media analysis. The rationale behind the project was concerns that within asylum-seeking populations, there were fighting-age males coming off of boats that looked healthy, which could be a cause for concern in regard to ISIS. IBM created an experimental terrorist credit score to weed out ISIS fighters from refugees.

New technical systems of state control use refugees as test cases, including AI systems that classify individuals based on social behavior and creditworthiness. These systems harvest personal data from refugees and can influence access to loans, permission to cross borders, and even public benefits. Such scoring logic is deeply entwined in law enforcement, border control, and welfare state functions and can further disadvantage already marginalized populations.

Former Republican Governor of Michigan, Rick Snyder, implemented two algorithm-driven austerity programs that aimed to disqualify individuals from food assistance and unemployment insurance. The programs inaccurately matched thousands of Michigan residents and led to severe consequences, including seizure of tax refunds, garnishment of wages, and civil penalties. The systems were financial failures and resulted in successful lawsuits against the state. The targeting of fugitive felons and suspected fraudsters reflects the consistent logic of state-driven AI systems.

Militaristic systems of command-and-control for punishment and exclusion undermine the social and economic stability they aim to promote. State bureaucracy and automated decision systems are designed on a threat-targeting model that evaluates, scores, and serves individuals and communities. The philosophy of data extraction for U.S. hegemony breaks down under scrutiny as planetary computation exceeds traditional state border and governance limits. The self-reinforcing logic of planetary-scale computation is far messier than the winner takes all idea of sovereign AI.

The idea that AI infrastructure is securely contained within national borders is a myth as the labor force behind it is hybrid and global. The AI and algorithmic systems used by the state reveal a covert philosophy of command and control through extractive data techniques and surveillance. The deep intermingling of state, municipal, and corporate logics through extractive planetary computation is an uncomfortable bargain. The Snowden archive shows the overlapping and contradictory logics of surveillance, and the danger of losing sight of other visions of social progress in the pursuit of centralized control.

The collaboration between the state and the commercial sector has resulted in the securitization of risk and fear, with the NSAs methods and tools filtering down to everyday life. AI systems are embedded in social, political, cultural, and economic worlds and are designed to discriminate and amplify hierarchies, reproducing and amplifying existing structural inequalities. AI systems are expressions of power that primarily benefit the states, institutions, and corporations that they serve, and they furthers the profound imbalance of power between agents of the state and the people they are meant to serve.

The standard narrative of AI often emphasizes algorithmic exceptionalism, portraying AI systems as smarter and more objective than their human creators. This is fueled by narratives of magic and mystification that surround AI's history, with games being a preferred testing ground since the 1950s. However, this narrative overlooks the capital investment and human labor involved in creating AI systems, as well as the potential for biases and ethical concerns.

Enchanted determinism is a central logic of machine learning that involves reducing complexity into a clean signal for accurate prediction. This logic is rooted in a belief that mathematical formalisms can help us understand humans and society, and it obscures power and closes off informed public discussion. Enchanted determinism has two dominant strands: tech utopianism that offers computational interventions as universal solutions, and tech dystopianism that blames algorithms for negative outcomes without acknowledging the social context.

The utopian and dystopian views of AI both have an ahistorical perspective that places technology as the center of power, ignoring systemic forces like neoliberalism, racial inequality, and labor exploitation. AI's ability to optimize outcomes through statistical analysis at scale, rather than otherworldly intelligence, is often misunderstood due to the ideology of Cartesian dualism in AI.

The political economies of AI infrastructure construction and their wider planetary consequences are important questions that should be considered. The engineering plan for Google's first data center in Oregon reveals how much of the technical vision of AI depends on public utilities and subsidies. AI's expansion has been publicly subsidized, from defense funding and federal research agencies to public utilities and tax breaks, but has been relentlessly privatized to produce enormous financial gains for a tiny minority at the top of the extraction pyramid.

The description of AI as an abstract computational cloud that ignores the material conditions of its production represents the dual operation of abstraction and extraction in information capitalism. This book explores the planetary infrastructure of AI as an extractive industry with a deep entanglement of technology, capital, and power. To understand the full life cycle of AI and the dynamics of power that drive it, one must go beyond conventional maps of AI and locate it in a wider landscape of global interconnected systems.

To understand AI, one needs to see the structures of power it serves and the extraction politics behind it. AI is constructed from crowdworker-labeled datasets, used for navigating drones and directing immigration police. The tech sector's highly energy-intensive infrastructures are fueled by rare earth minerals, water, coal, and oil, while labor extraction is prevalent in the industry. The larger supply chain for computation is opaque, and AI's carbon footprint is not fully accounted for. A multiscalar perspective is needed to contend with these overlapping regimes.

The future of work involves humans supporting and correcting AI systems. AI systems are used to surveil workers and increase control in the hands of employers, while exploiting differences in time and wages to speed up capital circuits. The harvesting of the real world has intensified to capture previously hard-to-reach spaces for data extraction.

Personal photos and online debates are scraped to train machine vision and natural language algorithms, which has become a normalized and moralized practice. Training sets become the epistemic foundation by which AI systems classify the world, but labeling taxonomies force a way of seeing onto the world while claiming scientific neutrality, making the entire practice of harvesting data and using it to train systems inherently political. Bias is a symptom of a normative logic used to determine how the world should be seen and evaluated, as seen in affect detection, which draws on controversial ideas about the relation of faces to emotions. Institutions have always classified people into identity categories, narrowing their identities and creating hierarchies.

The article discusses the reduction of complex human experiences into quantifiable data for machine learning, and the epistemological violence and essentialization involved in the process. It also highlights the issue of ground truth for AI systems, particularly in the context of state power and intelligence agencies' use of data for lethal drone strikes.

The article discusses the use of AI in military and surveillance contexts, and its deep interconnections with the tech sector and government interests. The expansion of surveillance and blurring of private contractors, law enforcement, and the tech sector is resulting in a radical redrawing of civic life that strengthens the centers of power. The article raises questions about the democratization of AI and the need for connected movements for justice.

The idea of democratizing AI to reduce power asymmetries is problematic as the current infrastructure and power dynamics of AI are highly centralized. While there are many AI ethics frameworks, they are primarily produced by economically developed countries with little representation from those most harmed by AI systems. Furthermore, ethical principles are rarely enforceable or accountable, and the focus is on ethical ends rather than ethical means of AI application. There is currently no formal professional governance structure or norms for AI, allowing tech companies to choose how to deploy technologies and decide what ethical AI means for the world. Tech companies rarely suffer consequences when their AI systems violate the law or ethical principles. A reckoning is needed in the technology sector.

Ethics alone cannot address the fundamental concerns raised by AI. Power must be the focus, and the interests of communities most affected by AI must be centered. The lived experiences of those who are disempowered, discriminated against, and harmed by AI systems should guide our understanding of AI ethics. We need to ask questions about labor conditions, inhumane treatment of immigrants, and environmental impact. The social contract for technology is often signed before its consequences are revealed, and this gap has only widened. A focus on justice across all systems is necessary.

Philosopher Achille Mbemb criticizes the idea that the negative consequences of knowledge systems were unforeseeable, as they always aimed to rationalize the world based on corporate logic. A new critique of technology is needed that goes beyond legal and technical restraints, and instead prioritizes a more just and sustainable world. This requires questioning where AI should be used and emphasizing why it ought to be applied, rather than assuming technological inevitability.

The informatics of domination seeks to subject everything to statistical prediction and profit accumulation. Refusal of technology-first approaches requires rejecting the idea that tools serving capital, militaries, and police are value-neutral calculators that can be applied everywhere. The greatest hope lies in growing justice movements that unite labor, climate, and data justice and challenge the structures of power that AI reinforces. To do so requires embracing alternative solidarities and a different politics of inhabiting the Earth that repairs and shares the planet. There are sustainable collective politics beyond value extraction and commons worth preserving.

Jeff Bezos' private aerospace company, Blue Origin, aims to build reusable rockets and lunar landers to shuttle astronauts and cargo to the Moon by 2024. However, the company's longer-term mission is to help bring about a future in which millions are living and working in space. The company motto is "Step by Step, Ferociously," and Bezos argues that moving out into space is necessary to avoid capping population and energy usage per capita on Earth. The company's promotional video features inspirational images and a voiceover from Bezos, emphasizing the importance of space travel.

Jeff Bezos and other tech billionaires, such as Peter Diamandis and Elon Musk, are focused on space exploration and colonization as a way to address Earth's limited resources and energy demands. Bezos' company, Blue Origin, aims to build giant space colonies for people to live in floating environments, while heavy industry would move off-planet. Musk has also advocated for terraforming Mars and exploding nuclear weapons at its poles. The ideology behind these space efforts is interconnected with a belief in unlimited growth and a market-driven approach to solving problems.

The extreme wealth and power of the AI industry enables a small group of men to pursue their own private space race, relying on the knowledge and infrastructure of public space programs and government funding. Their aim is to extend extraction and growth across the solar system, driven by an imaginary of space, endless growth, and immortality. Jeff Bezos' plan for Blue Origin is inspired by a fantasy of space colonization from the 1970s, and the physicist and science fiction novelist Gerard K. O'Neill, who was driven by dismay about the limits to growth on Earth. The Club of Rome's 1972 report showed that resource and population collapse was likely before the year 2100, even with double the known resource reserves, and that assuming unlimited resources would lead to even more sharply collapsing populations due to pollution.

Limits to Growth suggested sustainable management and narrowing the gap between rich and poor nations as key to long-term stability, but did not foresee the global economy's interconnected systems and uneconomic forms of mining incentivized, driving environmental harm and accelerated resource depletion. The High Frontier redirected anxiety over gasoline shortages with visions of stable space structures to preserve the status quo and offer new opportunities, but without details of how it would work or be funded. The common corporate fantasies of tech billionaires for space colonization and frontier mining underscore a troubling relationship to Earth, seeking to displace Earth's population and capture territory for mineral extraction, and echo settler colonialism.

The Commercial Space Launch Competitiveness Act allows commercial space companies to own any resources extracted from asteroids and keep the profits, which contradicts the idea of space as a common interest. The pursuit of space colonization is driven by fear of death and time running out, and is a part of a larger trend of dark utopianism in Silicon Valley. The history of space exploration involves the launch of the first rocket containing a camera into space led by Wernher von Braun.

Wernher von Braun, chief rocket engineer of the Third Reich and a war criminal who used concentration camp slave labor to build V-2 rockets, is celebrated by Bezos as a hero in his Blue Origin commercial. Bezos owns three hundred thousand acres of land in an area with a violent colonial history, including a ranch he purchased in 2004.

The Blue Origin suborbital launch facility in West Texas is a guarded and gated private infrastructure-in-progress, where rockets are being tested for the company's first human mission into space. The landing pad is marked with a feather logo, and the area feels provisional and makeshift in the dry expanse of the Permian Basin. The facility is a technoscientific imaginary of power, extraction, and escape, driven by the wealthiest man on the planet as a hedge against Earth. The author is followed by two matching black Chevrolet pickups after taking a photograph and driving away.

The book "Atlas of AI" was many years in the making and thanks to the collective effort of friends, colleagues, collaborators, and coadventurers who contributed to its creation. The author expresses gratitude to those whose work left a deep imprint on the book, including Mike Ananny, Geoffrey Bowker, Benjamin Bratton, Simone Browne, Wendy Chun, Vladan Joler, Alondra Nelson, Jonathan Sterne, Lucy Suchman, Fred Turner, and McKenzie Wark, as well as multiple research communities, including Microsoft Research's FATE group and Social Media Collective.

The author expresses gratitude towards scholars, research assistants, archivists, and institutions that supported the creation of the AI Now Institute and the writing of their book. Specific individuals and organizations are mentioned by name, including those who helped with research, editing, and publishing.

The author expresses gratitude towards institutions and communities that welcomed them during their time as a chair in AI and Justice at the cole Normale Suprieure in Paris, a Richard von Weizscker Fellow at the Robert Bosch Academy in Berlin, and a Miengunyah Distinguished Visiting Fellow at the University of Melbourne. They also acknowledge feedback from various audiences at conferences, exhibitions, and lectures across different fields of study. The author also acknowledges the coauthors and journals with whom they collaborated in previously published articles that are included in the book.

The text lists various academic publications co-authored by the author related to the social and ethical implications of artificial intelligence (AI), including the need for regulation of facial recognition technology, the impact of biased data on predictive policing, the labor and resource implications of AI systems such as the Amazon Echo, the need for transparency in datasets, the potential harm of algorithmic bias, the ethical implications of autonomous experimentation in AI, and the need for human subjects in big data research.

The text lists various academic publications and collaborative reports co-authored by the author related to the politics, ethics, and impact of big data and artificial intelligence, including the need for a framework to address predictive privacy harms, the role of gender and race in discriminatory AI systems, and practical frameworks for public agency accountability. The author also acknowledges the support and contributions of various individuals, including Trevor Paglen, Vladan Joler, Laura Poitras, Karen Murphy, Adrian Hobbes, Edwina Throsby, and Bo Daley.

The text includes notes and references on various topics related to artificial intelligence and its history, including the story of Clever Hans, the dualisms of intelligence, critiques of AI approaches, the exclusionary culture of the tech industry, and discussions of the capabilities and limitations of deep networks. The text also includes references to works by Val Plumwood, Alan Turing, John von Neumann, Joseph Weizenbaum, and other scholars. It briefly mentions the controversy surrounding Marvin Minsky's association with Jeffrey Epstein and the culture of exclusion in the tech industry.

The text includes various notes and references on topics related to artificial intelligence, data colonization, epistemic cultures, and urban history. The author acknowledges various research communities that have informed their work, including those focused on fairness, accountability, transparency, and ethics in AI. The text also contains references to works by scholars who have written about the negative effects of technology on society, such as Ursula Franklin and Shoshana Zuboff. The text also includes references to San Francisco's history, such as its imperialistic past and the current issues of street homelessness.

The text contains various sources on topics related to geology, battery technology, AI, sustainable energy, and mineral extraction, including discussions on the geological features of the Silver Peak area in Nevada, the supply chain and possible drawbacks of Tesla's lithium-ion batteries, the role of extractivism in the AI industry, the risks and ethical concerns associated with the mining of conflict minerals, and the importance of responsible minerals policy and due diligence.

The text discusses the complex and often opaque supply chain of rare metals used in electronic devices, including the role of middlemen traders, ethical concerns related to mining practices, and the environmental impact of data centers and artificial intelligence (AI). It also references various sources that address these issues, such as responsible minerals sourcing policies, the history of undersea cables, the energy efficiency of data centers, and the impact of deep learning in natural language processing.

The text covers a range of topics including tech companies committing to carbon neutrality, the environmental impact of shipping, the concept of materiality, and labor issues related to data entry and automation. It references various sources and articles for each topic.



The text consists of a list of sources that discuss various topics related to labor and technology, including the establishment of supply chains for calculating engines, alternative work arrangements, worker surveillance, clock synchronization, and the relationship between technology and power. The sources range from academic papers to personal conversations with workers and authors.

This text discusses various topics including labor organizing and tech worker protests, the history of biometric identification systems, and the evolution of machine learning approaches. It also includes references to sources such as academic papers and conversations with experts.

The text consists of a list of references to various publications and studies related to artificial intelligence and data collection, including works on the history of computing, AI development, dataset creation and usage, and privacy concerns surrounding facial recognition technology. The author mentions their own participation in addressing the gap in dataset reporting and highlights other researchers' efforts in auditing AI models. The text also includes instances of controversial data collection practices and the subsequent removal of certain datasets by companies.

The text discusses various issues surrounding data collection, use, and metaphorical language in academic and industry settings. The author highlights examples of controversial data practices, including the deletion of certain datasets by companies and the unauthorized access of anonymous data. The text includes references to works that critique the use of certain data metaphors, such as data as a natural force or a resource to be consumed, and the potential harms of data colonialism. The author acknowledges the double-edged nature of the terminology and the ongoing impact of colonialism.

The text contains references to various sources on the topics of data citizenship, the use of data as capital, human subjects in big data research, predictive policing, data violence, and the history of extraction of data from marginalized communities. It also includes sources on the history of race and racism, including the mismeasurement of intelligence and the scientific basis for race.

The text discusses various issues related to bias in technology, including gender and racial bias in AI and machine learning, automated hiring practices, and discrimination in computation. It references sources that explore the history and implications of these issues and highlights efforts to address them, such as collaborations to accelerate fairness in AI research and the diversity in faces project. The text also references legal cases related to machine bias.

The text lists various sources and references related to research into artificial intelligence and its biases, including the problematic labeling of certain categories in the ImageNet dataset. The authors of the text have collaborated on a project called ImageNet Roulette, which allowed people to upload images and see how ImageNet labeled them, highlighting problematic labels while warning users in advance. The text also references efforts to create fairer datasets and the need to address biases in AI systems.

The text discusses various issues related to AI, including biased datasets, the use of ImageNet and facial recognition technology, and emotional detection and recognition. It highlights the problematic labeling of certain categories in ImageNet and the potential harm caused by biased datasets. It also examines the flaws in emotional expression detection by AI and the use of facial recognition technology in various fields, including hiring and surveillance.

The text is a list of references and citations related to the study of emotions, affect, and facial expressions. It includes works by various authors such as Tomkins, Ekman, Sedgwick, and Aristotle, among others. The references cover topics such as the relationship between emotions and cognition, the universality of facial expressions, and the history of physiognomy.

The text contains a list of references related to the study of human facial expressions and emotions, including works by Duchenne, Darwin, and Ekman. It also includes references to research on lie detection, artificial vision for robots, and comprehensive databases for facial expression analysis. Several specific studies and datasets are mentioned, including the Cohn-Kanade Dataset and the Affectiva-MIT Facial Expression Dataset.

The text contains a list of references and sources related to the study of emotions and facial expressions, including Paul Ekman's Facial Action Coding System (FACS), critiques and defenses of his work, and concerns around bias in technology-based emotion recognition. It also includes references to other scholars and their perspectives on the nature of emotions and their expression.

The text is a list of references related to the use of technology in surveillance and intelligence gathering, including facial recognition, cyberwarfare, and AI. It also includes references to government strategies and reports on the use of technology for defense and strategic advantage, as well as critiques and concerns around issues of privacy, unlawful surveillance, and the role of technology in politics and power.

The US military strategy has evolved over time, with the Second Offset strategy in the 1970s and 1980s focusing on computational advances in analytics and logistics. However, as Russia and China adopt similar capabilities and deploy digital networks for warfare, there is anxiety to establish a new kind of strategic advantage. This has led to collaborations between the military and technology companies such as Google, with controversies surrounding the use of AI for military purposes.

Microsoft won a large-scale engineering project contract with the US military. The use of AI in military applications has raised ethical concerns, including adherence to the Principle of Distinction and the identification of an imminent threat. Technology companies, such as Palantir, have also faced controversy for their involvement in law enforcement and immigration raids. The migration of law enforcement to intelligence has occurred before the shift to predictive analytics.

The text contains a list of sources and references related to the use of surveillance technologies and algorithms by law enforcement and government agencies, including issues of privacy, bias, and the impact on civil liberties. It also includes references to specific cases of surveillance and its consequences, as well as the role of big data and cloud computing in this context. The future of the Snowden archive is mentioned as uncertain.

The text discusses the closure of the Snowden archive and includes a list of references on various topics related to AI, including the use of games as a measure of intelligence, the cultural implications of AI, and the potential dangers of superintelligence. It also includes a reference to Google's shadow workforce.

The text contains a list of references to various sources discussing topics such as data as capital, AI ethics, algorithmic warfare, and space exploration by billionaires such as Jeff Bezos and Elon Musk. It also includes references to works addressing issues of decolonization, design justice, and race after technology.

The text discusses various references related to space exploration and colonization, including the Club of Rome's models, the limits to growth movement, and the Outer Space Treaty of 1967. It also refers to the potential risks of space mining and consumption of resources, as seen in Philip K. Dick's short story "Autofac," and the U.S. Commercial Space Launch Competitiveness Act.

The text includes various citations and a bibliography related to technology and its impact on society. It discusses automation, technological immortality, rare metals, state-run lithium industry in Bolivia, US drone strikes, and the use of AI in analyzing human states. The bibliography includes works on the internet, computation, and artificial intelligence.

The text consists of a list of various sources related to artificial intelligence, including articles, reports, and market data. The sources cover topics such as surveillance, automation, lithium mining, and the use of AI in hiring and face analysis. There are also references to specific companies such as Palantir and Amazon.

The sources cited cover various topics related to communication technology and its impact on society. These topics include automated media, age discrimination in job ads on Facebook, machine bias in criminal sentencing, the use of Palantir technology by ICE to detain undocumented immigrants, biographical engineering, Apple's commitment to carbon neutrality, supplier responsibility, territories of extraction under late capitalism, and the assessment of the Dodd-Frank Act on conflict minerals.

The text is a list of various sources and references on different topics, including conflict minerals, digital labor, Nietzsche's philosophy of science, speech recognition and understanding, emotions, homosexuality and psychiatry, and artificial intelligence on social media. No overarching summary can be provided.

The text is a list of references and citations covering various topics, including the struggle to define emotions, the impact of electric cars on metal resources, the global emissions footprint of ICT, the racial bias in machine learning, digital labor platforms and decent work, the use of Google AI in the Pentagon drone program, and Jeff Bezos' space endeavors.

The text includes various sources on topics such as space exploration, facial recognition, social class determination, historical events such as the eugenics movement and IBM's relationship with Nazi Germany, as well as literature and philosophy. There is also discussion on classification systems and memory practices in the sciences.

The text is a list of various books, articles, and reports related to technology, labor, surveillance, and military contracts, including "The Stack: On Software and Sovereignty," "Labor and Monopoly Capital: The Degradation of Work in the Twentieth Century," "Big Data Surveillance: The Case of Policing," and "Google Wants to Do Business with the Military - Many of Its Employees Don't."

The text includes various sources on technology and its impact on society, including articles on lithium-ion batteries, gender bias in commercial gender classification, violence in the Congo, sustainable energy, Apple's acquisition of an AI emotion reading company, the CalGang criminal intelligence system, Google's involvement with the Pentagon's AI drone program, and the relationship between power and responsibility in AI. It also includes a historical article from 1945 by Vannevar Bush on the potential of technology.

The text includes various sources on topics such as the NSA Utah Data Center's water usage, the AI race for strategic advantage, the supply of lithium to Tesla by a Chinese company, the racist history behind facial recognition technology, and the ecology of attention. The sources cited include academic articles, news reports, and speeches, among others.

The text contains a list of various academic articles, news articles, and book references covering topics such as history, climate change, neurology, disease classifications, and technology.

The text contains a list of various sources on topics such as crisis reporting, facial recognition in schools, lost shipping containers, green internet, Amazon's Alexa feature, and new materialism. It also includes references to books on topics such as design justice, data colonialism, and the costs of connection.

This text consists of a list of sources on various topics, including facial recognition technology, emotional experience and expression, artificial intelligence, and journalism.

The given text is a list of various sources covering topics such as technology, space exploration, judicial decisions, artificial intelligence, and human emotions. It includes sources such as reports, articles, books, and videos, covering a wide range of subjects.

The given text is a list of various sources covering topics such as artificial intelligence, image databases, machine learning bias, and the intersection of AI and climate change. It includes sources such as conference papers, government memoranda, magazine articles, and books, covering a wide range of subjects.

The provided link does not contain any coherent text or article for a concise summary to be generated. Instead, it includes a list of various sources, including books, articles, blogs, and dissertations, which may have discussed topics such as AI, climate change, nuclear disaster, depathologizing homosexuality, and AI cameras.

The text contains a list of sources on various topics, including the discourse in Cold War America, technopolitics of identity in apartheid South Africa, the use of the master-slave analogy in technical literature, and the study of emotions, particularly facial expressions, lying, and nonverbal communication. The sources include works by Paul Ekman, such as "Telling Lies," "Emotions Revealed," and "Universal Facial Expressions of Emotion," as well as his collaboration with Wallace V. Friesen on the "Facial Action Coding System." Other sources include "Broken Metaphor" by Ron Eglash and "History and the Technopolitics of Identity" by Paul N. Edwards and Gabrielle Hecht.

This text contains a list of sources on the topics of facial expressions, emotion recognition, and technology. It includes studies by Ekman and others on the universality and cultural specificity of emotions and the ability to detect lying. It also includes sources on the use of technology for emotion detection and recognition, as well as the lifespan of electronics. One source discusses the UK police's facial recognition system.

The text is a list of various sources related to topics such as facial recognition technology, automation, and artificial intelligence, including news articles, academic papers, and books. The sources cover a range of issues such as the accuracy and failure rates of facial recognition technology, the history and politics of automation and technical expertise, the biases and discriminatory effects of high-tech tools, and the ethical implications of using AI and machine learning in society.

The text contains a list of sources with various topics, including leaked emails showing Google's expectation of lucrative military drone AI work, federal policy for human subject protection, the WordNet lexical database, the science of facial expression, Palantir CEO defending the company's relationship with government agencies, Jeff Bezos' master plan, and the construction of knowledge in artificial intelligence.

The text includes various sources covering topics such as robotization in the domestic sphere, surveillance technology, facial displays, and data mining. It includes works by authors such as Michel Foucault and Ursula Franklin, as well as databases and articles on multiple encounter datasets and profiling technology.

The text includes various sources and topics, including academic articles, government reports, news articles, and books, covering subjects such as rocket engine production, biometrics evaluations, homeless populations, shipping industry, data mining, and AI research.

No coherent article is provided for me to summarize. Please provide the relevant text that needs to be summarized.

The text is a list of various sources and references on topics such as Web 2.0, the Internet of Things, the role of humans behind AI, biased AI, time management, lithium extraction in Bolivia, and face recognition technology.

The text contains various articles, papers, and books on different topics such as AI ethics, surveillance, drone warfare, low-wage work, and the impact of technology on society. The sources provide insights into the implications of these issues and how they affect people's lives, with examples such as the killing of innocent people by the NSA's SKYNET program, the impact of AI algorithms on the criminal justice system, and the ethical considerations surrounding AI ethics guidelines.

The text is a list of various sources, including articles and books, discussing topics such as OpenAI's language processing technology, China's AI industry, feminism and technoscience, Google's carbon footprint, facial recognition algorithms, Amazon's coaching of law enforcement, and the fear propagated by Amazon's home security company, Ring.

The text contains a list of various sources, including articles and books, related to topics such as facial recognition, emotions, Enron scandal, gender inequality in computing, waste management, tin mining, data violence, and Jeff Bezos' income.

The text is a list of various sources on topics such as artificial intelligence, surveillance, cloud computing, and responsible minerals supply chains, without a clear connection or main focus.

The text is a list of various sources on topics such as AI ethics, automation, biometrics, and digital image processing, without a clear connection or main focus.

The text is a list of various academic articles, books, and news articles, covering topics such as affect detection, alternative work arrangements, the history of race and racism in America, technology and immigration, automatic gender recognition, machine learning, and email classification research.

The text is a list of various academic references spanning multiple fields such as artificial intelligence, geology, psychology, and literature. The references cover topics such as AI's abilities in chess, the breakdown of materials in Tesla batteries, the concept of race as a social construct, and the categorization of thought processes.

The text is a list of citations from various sources including academic papers, news articles, and a book. Topics covered include the history of text prediction, the use of databases for tracking gang activity, the role of women in early computer programming, the development of Aristotle's theory of animal classification, the rare earths trade in China, the deletion of face databases by tech companies, and the limitations of using the master's tools to dismantle the master's house.

The text includes various sources related to technology and surveillance, facial recognition, language processing, artificial intelligence, and machine learning. It also covers historical and sociological perspectives on technology and the development of computer systems. The sources come from academic journals, newspapers, blogs, and other publications.

The text contains a list of various sources on topics such as automation and robotics in industry, the use and impact of technology, the ethics of automating design, and Fordism in the fast food industry. Sources include articles from The New York Times, NBC News, and BBC Future, as well as books by Karl Marx and Shannon Mattern.

This text contains a list of various sources on diverse topics such as the sociology of health, biological thought, necropolitics, artificial intelligence, Fukushima disaster, facial expression dataset, racial justice on the internet, voice recognition, and reducing the risk of conflict.

The text contains various sources on different topics, including the limits to growth, facial expression research, analytical engines, big data ethics, connected devices, cultural construction of social agency, and contemporary capitalism.

The text contains a list of various sources, including academic papers, books, and news articles, related to topics such as artificial intelligence, surveillance, bias, and visual representation. Some of the sources mentioned include the book "Eyes in the Sky: The Secret Rise of Gorgon Stare and How It Will Watch Us All" by Arthur Holland Michel, the article "WeWork Just Made a Disturbing Acquisition; It Raises a Lot of Flags about Workers Privacy" by Betsy Mikel, and the academic paper "The Need for Biases in Learning Generalizations" by Tom M. Mitchell. The text does not provide a cohesive narrative or argument.

The text contains a list of various sources on topics such as advertisers fleeing YouTube due to child exploitation, the impact of technology on society, facial recognition technology, the NSA's surveillance practices, the Outer Space Treaty, and efforts to ensure fairness in AI. Additionally, it includes a scientific study on the mineral commodity supply risk of the US manufacturing sector.

The text contains a list of sources including research on fairness in AI, a special database, an article on gender bias in Apple Card, a guide on determining battery power rating, an app called Neighbors by Ring, books on DNA, race, and disability history, a manual on American English, an article on racism in science, and an article on surveillance help given to ICE.

The text consists of a list of various sources, including books, articles, videos, and websites on a range of topics, such as artificial intelligence, privacy, data ethics, space colonization, and legal cases.

The text contains a list of various sources, including articles, books, and websites, on topics such as artificial intelligence, data privacy, media, and military strategy. Some of the sources focus on specific companies and technologies, such as Palantir and Microsoft's AI bot Tay, while others explore broader societal issues, such as racism and inequality in big data and the impact of technology on mining practices in Congo.

The text includes a list of references covering various topics, such as face recognition technology, affective computing, AI principles, feminist logic, deep and shallow networks, lie detection, space colonization, renewable energy, and NSA growth.

The text is a list of various sources on topics related to technology and society, including artificial intelligence, face recognition technology, renewable energy in the cloud industry, labor in the iPhone era, terrorist assemblages, and the impact of technology on race and bias. It includes articles, books, and news sources from a variety of perspectives and disciplines.

The text contains a list of references to various articles and books on topics such as the history of computing, responsible mineral sourcing, the use of algorithms in healthcare and policing, and the collision probabilities of cars with planets.

The text contains a list of references to various articles and books on topics such as the use of algorithms in government surveillance, content moderation on social media, racism and sexism in space colonization, and legal challenges related to voice recording technology and data protection.

This text includes various sources on the topics of artificial intelligence, datafication, the internet, workplace datafication, facial recognition technology, NSA surveillance, AI regulation, Babbage's calculating engines, and space settlements.



Eric Schmidt warns that Silicon Valley could lose to China in the field of artificial intelligence. 

Algorithms have been taught to see identity, constructing race and gender in image databases for facial analysis. 

There is a global shortage of electric vehicle battery minerals, which could affect the production of electric cars. 

Shipping is one of the largest greenhouse gas emitters in the world. 

The NSA targets users' online anonymity by attacking Tor. 

Facial recognition software is becoming more advanced in reading human emotions. 

Certain schemes to improve the human condition have failed, as discussed in "Seeing Like a State" by James C. Scott. 

"Shame and Its Sisters" by Silvan Tomkins examines the role of shame in human psychology.

This is a list of various sources with different topics, including academic papers on computer vision and AI, news articles on business and technology, and policy briefs on video surveillance. Topics range from health insurance, asteroid mining, and lithium batteries, to disability theory and emotional categories.

This is a list of various sources covering different topics, including academic papers on responsible research with crowds and machine learning, news articles on technology and surveillance, and blogs on carbon neutrality and technology's impact on the military. Topics range from online job ads, deadly materials used in smartphones, and drone surveillance, to Go playing without human knowledge and Microsoft's carbon negative commitment. The Snowden Archive is also included in the list.

The text includes various sources discussing topics such as facial recognition technology and its ethical implications, the history and impact of beef production in America, the use of data in contemporary culture, the development of artificial intelligence and its potential dangers, the concept of the panopticon, and the business strategies and competition between Microsoft and Amazon.

The text contains a list of various sources covering topics such as Microsoft's win over Amazon in a cloud deal, a brainwash dataset, the Northern Virginia data center market, algorithmic warfare, street homelessness, and more. There is no clear theme or connection between the sources.

No discernible text found. The provided text appears to be a list of various sources with no clear connection or context.

The text is a list of references to various articles and publications covering a range of topics including renewable energy, real-time analytics in supply chains, 17th-century sound technology, Facebook's use of AI to combat hate speech in Myanmar, IBM's software for identifying refugees and terrorists, a Victorian ecological disaster, computing and intelligence, global collapse, geology, affective computing, and cognitive psychology.

This is a list of various sources, including academic articles, reports, and news articles on topics such as heuristics and biases, personal histories of technology, maritime transport, space launch competitiveness, covert actions in Chile, electricity generation, shipping pollution, AI emotion recognition, automated surveillance drones, and the computer and the brain.

This is a list of sources cited in an unknown text, covering topics such as Tesla's electric cars, the role of Silicon Valley in shaping time, Google's use of temporary workers, Palantir's data gathering and analysis capabilities, facial recognition technology, immigration policies under the Trump administration, the life of Wernher von Braun, and the relationship between computers and human reasoning.

The text includes various sources covering topics such as the impact of computers on society, Elon Musk's thoughts on the risks of journeying to Mars, architecture in eighteenth century Russia, modern racism, fairness and power in AI, the importance of asteroids, innovations in shipping boxes, top lobbying victories of 2015, and a controversial new project in predictive policing.



 The text includes various sources related to technology and its impact on society, including topics such as predictive policing, AI bias, and surveillance capitalism. 

 It features academic works such as Langdon Winner's "The Whale and the Reactor" and Shoshana Zuboff's "The Age of Surveillance Capitalism." 

 It also includes news articles and reports on current events, such as the use of cheap labor for AI labeling in China and the filtering and balancing of datasets to promote fairness. 

 The sources come from various outlets and institutions, including The Verge, PBS NewsHour, the New York Times, and the Federal Energy Regulatory Commission.

The article lists various individuals, organizations, and concepts related to artificial intelligence (AI), including affect recognition, algorithmic assessment and scheduling, Amazon and Apple, and the use of data. The article also discusses issues such as bias and surveillance in AI, as well as the carbon footprint and supply chain of technology companies.



 The text covers various topics related to artificial intelligence (AI), including its impact on human labor, logistical layers, power structures, and mineral extraction. 

 It discusses the biases inherent in AI systems, including cognitive, gender, racial, and statistical biases. 

 The text also mentions key figures in the history of AI, such as Charles Babbage and Vannevar Bush, as well as companies like Google, Baidu, and ByteDance. 

 It explores the use of biometrics and affect recognition technology, as well as state power and surveillance by organizations like the CIA and Palantir. 

 Finally, it touches on topics like cargo containers, lithium mining in Australia, Bolivia, and Mongolia, and the use of AI for affect recognition studies in Brazil.

The text covers various topics related to technology and society, including the development and implications of artificial intelligence (AI), data classification and bias, environmental impacts of technology, and historical and current issues related to mineral extraction and colonialism. It also discusses specific examples of AI technologies, such as facial recognition and crime assessment algorithms, and the ethical concerns associated with them. Overall, the text highlights the need for critical examination of the social and environmental impacts of technology and the importance of considering ethical implications in its development and use.

The text covers a variety of topics related to data, including data colonialism, privacy issues, algorithmic assessment, and the use of biometrics and facial recognition algorithms. It also explores the history of demand for data, the role of state power in data extraction and analysis, and the potential for discrimination and bias in these processes. Other topics discussed include the environmental impact of technology, the use of drones and machine learning in warfare, and the potential implications of automation and AI for the future of work.

The text discusses various topics related to AI and its impacts, including its use in extractive industries like gold and lithium mining, e-waste dumping, and cargo containers. It also explores issues of bias in algorithms used by companies like Facebook and the exploitation of labor in the development and deployment of AI. The text delves into facial recognition algorithms and affect recognition systems, as well as the ethical frameworks for AI and epistemic machinery. Finally, it touches on topics like submarine cable construction and the water supply, as well as the involvement of state power in the development of AI.

The text consists of a list of names and topics related to technology and society, including individuals such as Michel Foucault, Donna Haraway, and Grace Hopper, companies such as Google and Foxconn, and issues such as bias in data classification algorithms and the use of AI in government surveillance. The list includes references to various books, programs, and events, as well as scientific terms such as gadolinium and germanium.

The text contains a list of various topics, including individuals and companies involved in AI research, datasets, government initiatives, labor practices, and ethical considerations. Some specific points mentioned include the use of affect recognition by IBM, data classification schemes like ImageNet, institutional review boards, and the exploitation of labor through algorithmic assessment and surveillance. The text also briefly touches on topics such as climate justice and workplace automation.

This is a list of various people, organizations, and concepts related to the intersection of technology and society. It includes topics such as the extraction industry, law enforcement, artificial intelligence, materiality, and carbon footprints. Specific examples mentioned include Microsoft's involvement in facial recognition and carbon emissions, AI's need for mineral extraction, and the potential for affect recognition technology to be used in law enforcement.

The text includes references to various topics such as lithium mining in Mongolia, rare earth mineral extraction, the myth of clean tech, and surveillance by the National Security Agency. It also mentions individuals like Elon Musk, Lewis Mumford, and Evgeny Morozov. The discussion covers technologies such as the MS-Celeb dataset, mug shot databases, natural language processing models, and the Network Time Protocol. The text also refers to organizations like the National Science Foundation, the National Institute of Standards and Technology, and Palantir. Additionally, it touches upon topics such as necropolitics, the observer-expectancy effect, and the obsolescence cycle.

The text includes various topics related to technology and society, including historical figures and events, scientific concepts, and current issues. Some key points mentioned are: 

 The use of AI technology in power structures and its implications on society. 

 The impact of rare earth minerals and their use in technology. 

 Issues related to privacy and surveillance, including facial recognition and affect recognition systems. 

 The role of race in AI and data classification algorithms. 

 The tech industry in Silicon Valley and its impact on society. 

 The use of technology in the shipping industry. 

 Historical events such as the impact of gold mining on San Francisco and the role of railroads in time coordination.

The text mentions various topics including silver mining, Herbert Simon, Upton Sinclair, Adam Smith, the Snowden archive, social credit scoring, SpaceX, surveillance, state power, telegraph, TikTok, and transparency. It also discusses the role of AI in war, data extraction, and targeting, as well as labor issues and racial classification schemes in South Africa. Additionally, it covers the topics of space colonization, submarine cable construction, and the supply chain for companies like Tesla.

The text covers various topics including TrueTime, Alan Turing, Uber, Unilever, and WeWork. It also mentions different universities and research organizations such as the U.S. Army Research Laboratory, the University of California San Diego, and the U.S. Geological Survey. Additionally, it discusses different people such as Langdon Winner, Norbert Wiener, and Shoshana Zuboff and their works related to machine learning, object recognition, and surveillance capitalism. The text also includes topics such as tungsten, utopian vs. dystopian perspectives, and water supply.

